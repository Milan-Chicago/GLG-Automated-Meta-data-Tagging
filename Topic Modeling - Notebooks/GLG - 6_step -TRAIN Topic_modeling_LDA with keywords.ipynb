{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea:\n",
    "Our solution: LDA + keywords from clusters of BERT based embeddings of noun phrases and verbs :\n",
    "- Each noun phrase and verb in the texts is  transformed to embedding vector using Universal Sentence Encoder (transformer based on BERT)\n",
    "- Embedding vectors from (a) are clustered (HDBSCAN + UNET)\n",
    "- Words/phrases with embedding vectors closest to the centers of resulting clusters form key word/phrase\n",
    "- Each text in the training sample is converted to collection of key-phrases by replacing its noun phrases and verbs with keyword/phrases and deleting other words\n",
    "- LDA is performed on the transformed texts\n",
    "\n",
    "\n",
    "**Reference:**<br>\n",
    "- Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-CÃ©spedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. **Universal Sentence Encoder.** *arXiv:1803.11175, 2018.*\n",
    "- McInnes, L, Healy, J, **UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction**, *ArXiv e-prints 1802.03426, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clNUUp3MUO2t"
   },
   "source": [
    "# Load data and python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1622131163361,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "7dYQIbH6UO2u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
      "  from numpy.dual import register_func\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:864: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1120: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1148: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1620: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1754: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/Users/tatiana/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# topic modeling libraries\n",
    "import pyLDAvis.gensim \n",
    "\n",
    "# data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# topic modeling libraries\n",
    "from gensim import models, corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "# supporting libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import topic_modeling_v3 as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (33982, 18)\n",
      "df_train.columns: Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
      "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
      "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
      "       'group_level_1', 'group_level_2', 'group_level_3', 'all_words',\n",
      "       'all_key_words'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open(\"./transition_files/df_train_for_LDA.pickle\", 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "print(\"df_train.shape:\", df_train.shape)\n",
    "print(\"df_train.columns:\",df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data shape: (33982, 18)\n",
      "\n",
      "Number of unique key-words for topic modeling dictionary: 4330\n",
      "LDA dictionary file is saved to: ./output/lda_keywords/dictionary1.pickle\n",
      "\n",
      "Number of texts processed:  33982\n",
      "Number of extracted key-words:  4330\n",
      "\n",
      "Each text is represented by list of  4330  tuples: \n",
      "\t\t(key-words's index in bag-of-words dictionary, key-words's term frequency)\n",
      "Processing time in minutes: 0.05\n"
     ]
    }
   ],
   "source": [
    "#prepare data for LDA\n",
    "start_time = time.time()\n",
    "df_data_1 = tm.prepare_for_modeling(data_path=\"\", model_type=\"LDA-KeyWords\",\n",
    "                                    params={\"TEXT_prepared_df\": df_train,\n",
    "                                     \"save_LDA_dictionary_path\": \"./output/lda_keywords/dictionary1.pickle\",\n",
    "                                     \"text_column\": \"text\"\n",
    "                                     },\n",
    "                                    verbose=2)\n",
    "end_time = time.time()\n",
    "print(\"Processing time in minutes:\", round((end_time - start_time)/60,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA with BERT-UMAP-HDBSCAN clustered KeyWords (NOUN_PHRASEs and VERBs)\n",
      "loaded data shape: (33982, 19)\n",
      "\n",
      "Creating document-term matrix for LDA...\n",
      "\n",
      "Training LDA model with  10  topics...\n",
      "LDA model file is saved to: ./output/lda_keywords/LDA_model1\n",
      "Top topic indexes are selected. NOTE \"-1\" corresponds to top topic with probability < 20%\n",
      "Processing time in minutes: 1.38\n"
     ]
    }
   ],
   "source": [
    "#first level of topics\n",
    "start_time = time.time()\n",
    "df_first_level = tm.train_model(model_type=\"LDA-KeyWords\",\n",
    "                            params={\"num_topics\": 10,\n",
    "                                    \"LDA_prepared_df\": df_data_1,\n",
    "                                    \"LDA_dictionary_path\": \"./output/lda_keywords/dictionary1.pickle\",\n",
    "                                    \"save_LDA_model_path\": \"./output/lda_keywords/LDA_model1\"\n",
    "                                    },\n",
    "                               verbose=2)\n",
    "end_time = time.time()\n",
    "print(\"Processing time in minutes:\", round((end_time - start_time)/60,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        2\n",
       "2        5\n",
       "3     5593\n",
       "4     1050\n",
       "5     4065\n",
       "6     1368\n",
       "7    13293\n",
       "8     8598\n",
       "9        7\n",
       "Name: first_level_topic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#value count of TOP level topics\n",
    "df_first_level['first_level_topic'] = df_first_level['top_topic']\n",
    "df_first_level['first_level_topic_proba'] = df_first_level['top_topic_proba']\n",
    "df_first_level['first_level_topic'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
       "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
       "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
       "       'group_level_1', 'group_level_2', 'group_level_3', 'all_words',\n",
       "       'all_key_words', 'doc2bow', 'infered_topics', 'top_topic',\n",
       "       'top_topic_proba', 'first_level_topic', 'first_level_topic_proba'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first_level.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_first_level[df_first_level['first_level_topic'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_first_level[df_first_level['first_level_topic'] == 0]['first_10_sents'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_first_level[df_first_level['first_level_topic'] == 0]['all_key_words'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_level = df_first_level.drop(columns=['doc2bow',\n",
    "       'infered_topics', 'top_topic', 'top_topic_proba'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Get SECOND level topics (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_level_topics = list(set(df_first_level['first_level_topic']))\n",
    "first_level_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected topic index: 0\n",
      "Training LDA with BERT-UMAP-HDBSCAN clustered KeyWords (NOUN_PHRASEs and VERBs)\n",
      "LDA model file is saved to: ./output/lda_keywords/LDA_model1_1\n",
      "\n",
      "Value counts of SECOND level topics:\n",
      "1    1\n",
      "Name: second_level_topic, dtype: int64\n",
      "##################################################\n",
      "\n",
      "Selected topic index: 1\n",
      "Training LDA with BERT-UMAP-HDBSCAN clustered KeyWords (NOUN_PHRASEs and VERBs)\n",
      "LDA model file is saved to: ./output/lda_keywords/LDA_model1_2\n",
      "\n",
      "Value counts of SECOND level topics:\n",
      "0    1\n",
      "3    1\n",
      "Name: second_level_topic, dtype: int64\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "list_dfs = []\n",
    "for topic in first_level_topics[:2]:\n",
    "    print(\"\\nSelected topic index:\", topic)\n",
    "    df_topic = df_first_level[df_first_level['first_level_topic'] == topic].copy()\n",
    "    save_dict_path = \"./output/lda_keywords/dictionary1_\"+str(topic+1)+\".pickle\"\n",
    "    save_LDA_model_path = \"./output/lda_keywords/LDA_model1_\" + str(topic + 1)\n",
    "    \n",
    "    df_data_tmp = tm.prepare_for_modeling(data_path=\"\", model_type=\"LDA-KeyWords\",\n",
    "                                       params={\"TEXT_prepared_df\": df_topic,\n",
    "                                               \"save_LDA_dictionary_path\": save_dict_path\n",
    "                                               },\n",
    "                                       verbose=1)\n",
    "\n",
    "    df_2nd_tmp = tm.train_model(model_type=\"LDA-KeyWords\",\n",
    "                                params={\"num_topics\": 10,\n",
    "                                        \"LDA_prepared_df\": df_data_tmp,\n",
    "                                        \"LDA_dictionary_path\": save_dict_path,\n",
    "                                        \"save_LDA_model_path\": save_LDA_model_path\n",
    "                                        },\n",
    "                                verbose=1)\n",
    "\n",
    "    #value counts of SECOND level topics\n",
    "    print(\"\\nValue counts of SECOND level topics:\")\n",
    "    df_2nd_tmp['second_level_topic'] = df_2nd_tmp['top_topic']\n",
    "    df_2nd_tmp['second_level_topic_proba'] = df_2nd_tmp['top_topic_proba']\n",
    "    print(df_2nd_tmp['second_level_topic'].value_counts().sort_index())\n",
    "\n",
    "    print(\"#\"*50)\n",
    "    df_2nd_tmp = df_2nd_tmp.drop(columns=['doc2bow',\n",
    "                                           'infered_topics', 'top_topic', 'top_topic_proba'])\n",
    "    list_dfs.append(df_2nd_tmp)\n",
    "finish = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of gettig Second level topics in minutes: 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
       "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
       "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
       "       'group_level_1', 'group_level_2', 'group_level_3', 'all_words',\n",
       "       'all_key_words', 'first_level_topic', 'first_level_topic_proba',\n",
       "       'second_level_topic', 'second_level_topic_proba'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Time of gettig Second level topics in minutes:\", round((finish-start)/60,2))\n",
    "df_second_level = pd.concat(list_dfs)\n",
    "df_second_level.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Get THIRD level topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_level_topic</th>\n",
       "      <th>first_level_topic_proba</th>\n",
       "      <th>second_level_topic</th>\n",
       "      <th>second_level_topic_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.614413</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.886535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.175463</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>0.161597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.556282</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.699982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715329</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.979802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730676</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.983928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       first_level_topic  first_level_topic_proba  second_level_topic  \\\n",
       "count           3.000000                 3.000000            3.000000   \n",
       "mean            0.666667                 0.614413            1.333333   \n",
       "std             0.577350                 0.175463            1.527525   \n",
       "min             0.000000                 0.412583            0.000000   \n",
       "25%             0.500000                 0.556282            0.500000   \n",
       "50%             1.000000                 0.699982            1.000000   \n",
       "75%             1.000000                 0.715329            2.000000   \n",
       "max             1.000000                 0.730676            3.000000   \n",
       "\n",
       "       second_level_topic_proba  \n",
       "count                  3.000000  \n",
       "mean                   0.886535  \n",
       "std                    0.161597  \n",
       "min                    0.700000  \n",
       "25%                    0.837838  \n",
       "50%                    0.975676  \n",
       "75%                    0.979802  \n",
       "max                    0.983928  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_second_level[['first_level_topic',\n",
    "       'first_level_topic_proba', 'second_level_topic',\n",
    "       'second_level_topic_proba']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected FIRST level topic index: 0\n",
      "second_level_topics [1]\n",
      "\n",
      "Selected topics' indexes: (0, 1)\n",
      "Training LDA with BERT-UMAP-HDBSCAN clustered KeyWords (NOUN_PHRASEs and VERBs)\n",
      "LDA model file is saved to: ./output/lda_keywords/LDA_model1_1_2\n",
      "\n",
      "Value counts of SECOND level topics:\n",
      "1    1\n",
      "Name: second_level_topic, dtype: int64\n",
      "##################################################\n",
      "\n",
      "Selected FIRST level topic index: 1\n",
      "second_level_topics [0, 3]\n",
      "\n",
      "Selected topics' indexes: (1, 0)\n",
      "Training LDA with BERT-UMAP-HDBSCAN clustered KeyWords (NOUN_PHRASEs and VERBs)\n",
      "LDA model file is saved to: ./output/lda_keywords/LDA_model1_2_1\n",
      "\n",
      "Value counts of SECOND level topics:\n",
      "0    1\n",
      "Name: second_level_topic, dtype: int64\n",
      "##################################################\n",
      "\n",
      "Selected topics' indexes: (1, 3)\n",
      "Training LDA with BERT-UMAP-HDBSCAN clustered KeyWords (NOUN_PHRASEs and VERBs)\n",
      "LDA model file is saved to: ./output/lda_keywords/LDA_model1_2_4\n",
      "\n",
      "Value counts of SECOND level topics:\n",
      "3    1\n",
      "Name: second_level_topic, dtype: int64\n",
      "##################################################\n",
      "\n",
      "Selected FIRST level topic index: 2\n",
      "second_level_topics []\n",
      "\n",
      "Selected FIRST level topic index: 3\n",
      "second_level_topics []\n",
      "\n",
      "Selected FIRST level topic index: 4\n",
      "second_level_topics []\n",
      "\n",
      "Selected FIRST level topic index: 5\n",
      "second_level_topics []\n",
      "\n",
      "Selected FIRST level topic index: 6\n",
      "second_level_topics []\n",
      "\n",
      "Selected FIRST level topic index: 7\n",
      "second_level_topics []\n",
      "\n",
      "Selected FIRST level topic index: 8\n",
      "second_level_topics []\n",
      "\n",
      "Selected FIRST level topic index: 9\n",
      "second_level_topics []\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "list_dfs = []\n",
    "\n",
    "for topic_1st in first_level_topics:\n",
    "    print(\"\\nSelected FIRST level topic index:\",topic_1st)\n",
    "    df_1st_tmp = df_second_level[df_second_level['first_level_topic'] == topic_1st].copy()\n",
    "    second_level_topics = list(set(df_1st_tmp['second_level_topic']))\n",
    "    print(\"second_level_topics\", second_level_topics)\n",
    "    \n",
    "    for topic_2nd in second_level_topics:\n",
    "        print(\"\\nSelected topics' indexes:\", (topic_1st, topic_2nd))\n",
    "        \n",
    "        save_dict_path = \"./output/lda_keywords/dictionary1_\"+str(topic_1st+1)+\"_\"+str(topic_2nd+1)+\".pickle\"\n",
    "        save_LDA_model_path = \"./output/lda_keywords/LDA_model1_\"+str(topic_1st+1)+\"_\"+str(topic_2nd+1)\n",
    "        \n",
    "        df_2nd_tmp = df_1st_tmp[df_1st_tmp['second_level_topic'] == topic_2nd].copy()\n",
    "        \n",
    "        df_data_tmp = tm.prepare_for_modeling(data_path=\"\", model_type=\"LDA-KeyWords\",\n",
    "                                           params={\"TEXT_prepared_df\": df_2nd_tmp,\n",
    "                                                   \"save_LDA_dictionary_path\": save_dict_path\n",
    "                                                   },\n",
    "                                           verbose=1)\n",
    "\n",
    "        df_3d_tmp = tm.train_model(model_type=\"LDA-KeyWords\",\n",
    "                                    params={\"num_topics\": 10,\n",
    "                                            \"LDA_prepared_df\": df_data_tmp,\n",
    "                                            \"LDA_dictionary_path\": save_dict_path,\n",
    "                                            \"save_LDA_model_path\": save_LDA_model_path,\n",
    "                                            },\n",
    "                                    verbose=1)\n",
    "\n",
    "        #value counts of SECOND level topics\n",
    "        print(\"\\nValue counts of SECOND level topics:\")\n",
    "        df_3d_tmp['third_level_topic'] = df_3d_tmp['top_topic']\n",
    "        df_3d_tmp['third_level_topic_proba'] = df_3d_tmp['top_topic_proba']\n",
    "        print(df_3d_tmp['second_level_topic'].value_counts().sort_index())\n",
    "\n",
    "        print(\"#\"*50)\n",
    "        df_3d_tmp = df_3d_tmp.drop(columns=['doc2bow',\n",
    "                                               'infered_topics', 'top_topic', 'top_topic_proba'])\n",
    "        list_dfs.append(df_3d_tmp)\n",
    "finish = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of gettig Third level topics in minutes: 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
       "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
       "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
       "       'group_level_1', 'group_level_2', 'group_level_3', 'all_words',\n",
       "       'all_key_words', 'first_level_topic', 'first_level_topic_proba',\n",
       "       'second_level_topic', 'second_level_topic_proba', 'third_level_topic',\n",
       "       'third_level_topic_proba'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Time of gettig Third level topics in minutes:\", round((finish-start)/60,2))\n",
    "df_third_level = pd.concat(list_dfs)\n",
    "df_third_level.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_third_level.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Topics (as a most frequent noun in the cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <td>Wired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_level_topic</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second_level_topic</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third_level_topic</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      22590\n",
       "publication           Wired\n",
       "section             culture\n",
       "first_level_topic       0  \n",
       "second_level_topic      1  \n",
       "third_level_topic       1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tm.get_topic_names(df_result, 'first_level_topic', 'list_of_nouns')\n",
    "df['second_level_topic'] = tm.get_topic_names(df_result, \n",
    "                                              'second_level_topic', 'list_of_nouns')['second_level_topic']\n",
    "df['third_level_topic'] = tm.get_topic_names(df_result, \n",
    "                                             'third_level_topic', 'list_of_nouns')['third_level_topic']\n",
    "df[['publication', \n",
    "    'section',\n",
    "    'first_level_topic',\n",
    "    'second_level_topic',\n",
    "    'third_level_topic'\n",
    "   ]].iloc[::1000].head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For test:\n",
    "1) extract noun_phrases_lemmatised and verb_lemmas from text\n",
    "\n",
    "2) For Each word from (1):\n",
    "- get text emmbedings\n",
    "- pretrained UNET -> reduced dimentions\n",
    "- get clusters from pretrained HDBSCSAN clustering\n",
    "- get claster label (keyWords)\n",
    "\n",
    "3) replace text with keyWords\n",
    "4) get topics from pretrained LDA"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GLG - 3_step - Assigning Text Groups.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.45px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
