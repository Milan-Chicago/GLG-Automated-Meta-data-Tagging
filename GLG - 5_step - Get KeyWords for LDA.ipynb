{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea:\n",
    "Our solution: LDA + keywords from clusters of BERT based embeddings of noun phrases and verbs :\n",
    "- Each noun phrase and verb in the texts is  transformed to embedding vector using Universal Sentence Encoder (transformer based on BERT)\n",
    "- Embedding vectors from (a) are clustered (HDBSCAN + UNET)\n",
    "- Words/phrases with embedding vectors closest to the centers of resulting clusters form key word/phrase\n",
    "- Each text in the training sample is converted to collection of key-phrases by replacing its noun phrases and verbs with keyword/phrases and deleting other words\n",
    "- LDA is performed on the transformed texts\n",
    "\n",
    "\n",
    "**Reference:**<br>\n",
    "- Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-CÃ©spedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. **Universal Sentence Encoder.** *arXiv:1803.11175, 2018.*\n",
    "- McInnes, L, Healy, J, **UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction**, *ArXiv e-prints 1802.03426, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clNUUp3MUO2t"
   },
   "source": [
    "# Load data and python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1622131163361,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "7dYQIbH6UO2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "module https://tfhub.dev/google/universal-sentence-encoder-large/5 loaded\n"
     ]
    }
   ],
   "source": [
    "# data processing libraries\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# display wider columns in pandas data frames where necessary\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "#Load the Universal Sentence Encoder's TF Hub module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (33982, 16)\n",
      "df_train.shape: Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
      "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
      "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
      "       'group_level_1', 'group_level_2', 'group_level_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data/train_grouped.tsv\", sep=\"\\t\")\n",
    "print(\"df_train.shape:\", df_train.shape)\n",
    "print(\"df_train.shape:\",df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bxk10XYUqNp"
   },
   "source": [
    "# Getting text clusters through sentence embedding comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1622131190534,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "MsgF9abaX2Bn"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(df_data, column = \"word\", N_batches=1):\n",
    "    #split data into N batches\n",
    "    N = N_batches\n",
    "\n",
    "    part = int(len(df_data)/N)\n",
    "    print(N, \"batches with\", part + 1, column + \"s each\")\n",
    "\n",
    "    #get embeddings for each N words\n",
    "    index = 0\n",
    "    batch_num = 0\n",
    "    list_dfs = []\n",
    "\n",
    "    while index < len(df_data): \n",
    "        df_tmp = df_data.iloc[index : index + part].copy()\n",
    "        df_tmp = df_tmp.reset_index(drop=True)\n",
    "        print (\"Batch number:\", batch_num + 1, \"out of \", N)\n",
    "\n",
    "        df_batch_embeddings = pd.DataFrame(get_embeddings(list(df_tmp[column])).numpy())\n",
    "\n",
    "        num_embeddings = df_batch_embeddings.shape[1]\n",
    "        columns = [\"emb_\" + str(i) for i in range(512)]\n",
    "        df_tmp[columns] = df_batch_embeddings\n",
    "\n",
    "        list_dfs.append(df_tmp)\n",
    "        batch_num = batch_num + 1\n",
    "        index = index + part\n",
    "\n",
    "    #concatinate batches into single dataset\n",
    "    df_emb = pd.concat(list_dfs)\n",
    "\n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [rise, big emerging economy, china, india, steady march, globalisation, surge, number, people, business, tourism, result, demand, visa, unpreceden...\n",
       "1    [pfizer, commitment, corporate social responsibility csr, drugs giant talk, responsibility, society, world, access, product, work, ngos, global he...\n",
       "2    [week, federal reserve, interest rate, time, year, world, central bank, rate, recent year, long spell, course, chart, outcome, americas rate rise,...\n",
       "3    [cruise line, wave, year, nearly, holiday, sea, result, december 18th carnival, worlds largest operator, global market, fullyear earning, demand, ...\n",
       "4    [investors, calendar year, buoyant mood, unexpected event, consensus, respect, view, investor, market price, column, potential surprise, definitio...\n",
       "Name: noun_phrases, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['noun_phrases'] = df_train['noun_phrases'].str[2:-2]\n",
    "df_train['noun_phrases'] = df_train['noun_phrases'].str.lower().str.split(\"', '\")\n",
    "df_train['noun_phrases'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['rise', 'big emerging economy', 'china', 'india', 'steady march'], 1417049)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_NPs = list(df_train['noun_phrases'])\n",
    "all_NPs = [np for l in all_NPs for np in l if len(np)>0]\n",
    "all_NPs[:5], len(all_NPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[emerging, led, wanting, travel, granted, Upgrade, travel, apply, submit, streamline, scrap]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['list_of_verb_lemmas'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                               [merging, led, wanting, travel, granted, upgrade, travel, apply, submit, streamline, scra]\n",
       "1    [rided, embracing, insists, gain, strengthen, improve, deterred, seeking, intends, shift, domiciled, rejoiced, saved, paid, outraged, promised, im...\n",
       "2    [aised, ended, celebrate, tried, lift, forced, reverse, cut, help, understand, upgrade, strike, wish, save, spend, try, escape, slashing, encourag...\n",
       "3      [race, booked, improve, announced, control, demand, peaking, piling, based, got, moving, upgrade, increase, announced, establish, aimed, based, ad]\n",
       "4    [tart, caught, proved, reflected, like, suggest, judged, betting, expect, upgrade, weakens, having, pushed, tighten, buy, priced, doubt, tighten, ...\n",
       "Name: list_of_verb_lemmas, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['list_of_verb_lemmas'] = df_train['list_of_verb_lemmas'].str[2:-2]\n",
    "df_train['list_of_verb_lemmas'] = df_train['list_of_verb_lemmas'].str.lower().str.split(\", \")\n",
    "df_train['list_of_verb_lemmas'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['merging', 'led', 'wanting', 'travel', 'granted'], 675330)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_Vs = list(df_train['list_of_verb_lemmas'])\n",
    "all_Vs = [v for l in all_Vs for v in l if len(v)>0]\n",
    "all_Vs[:5], len(all_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419327"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words =  list(set(all_NPs + all_Vs))\n",
    "len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_QElB-4UM2z",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alphabets selfdrive division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social media website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mr trumps announcement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mike finn wolfhard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skincare business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           word\n",
       "0  alphabets selfdrive division\n",
       "1          social media website\n",
       "2        mr trumps announcement\n",
       "3            mike finn wolfhard\n",
       "4             skincare business"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.DataFrame({'word': all_words})\n",
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 batches with 4194 words each\n",
      "Batch number: 1 out of  100\n",
      "Batch number: 2 out of  100\n",
      "Batch number: 3 out of  100\n",
      "Batch number: 4 out of  100\n",
      "Batch number: 5 out of  100\n",
      "Batch number: 6 out of  100\n",
      "Batch number: 7 out of  100\n",
      "Batch number: 8 out of  100\n",
      "Batch number: 9 out of  100\n",
      "Batch number: 10 out of  100\n",
      "Batch number: 11 out of  100\n",
      "Batch number: 12 out of  100\n",
      "Batch number: 13 out of  100\n",
      "Batch number: 14 out of  100\n",
      "Batch number: 15 out of  100\n",
      "Batch number: 16 out of  100\n",
      "Batch number: 17 out of  100\n",
      "Batch number: 18 out of  100\n",
      "Batch number: 19 out of  100\n",
      "Batch number: 20 out of  100\n",
      "Batch number: 21 out of  100\n",
      "Batch number: 22 out of  100\n",
      "Batch number: 23 out of  100\n",
      "Batch number: 24 out of  100\n",
      "Batch number: 25 out of  100\n",
      "Batch number: 26 out of  100\n",
      "Batch number: 27 out of  100\n",
      "Batch number: 28 out of  100\n",
      "Batch number: 29 out of  100\n",
      "Batch number: 30 out of  100\n",
      "Batch number: 31 out of  100\n",
      "Batch number: 32 out of  100\n",
      "Batch number: 33 out of  100\n",
      "Batch number: 34 out of  100\n",
      "Batch number: 35 out of  100\n",
      "Batch number: 36 out of  100\n",
      "Batch number: 37 out of  100\n",
      "Batch number: 38 out of  100\n",
      "Batch number: 39 out of  100\n",
      "Batch number: 40 out of  100\n",
      "Batch number: 41 out of  100\n",
      "Batch number: 42 out of  100\n",
      "Batch number: 43 out of  100\n",
      "Batch number: 44 out of  100\n",
      "Batch number: 45 out of  100\n",
      "Batch number: 46 out of  100\n",
      "Batch number: 47 out of  100\n",
      "Batch number: 48 out of  100\n",
      "Batch number: 49 out of  100\n",
      "Batch number: 50 out of  100\n",
      "Batch number: 51 out of  100\n",
      "Batch number: 52 out of  100\n",
      "Batch number: 53 out of  100\n",
      "Batch number: 54 out of  100\n",
      "Batch number: 55 out of  100\n",
      "Batch number: 56 out of  100\n",
      "Batch number: 57 out of  100\n",
      "Batch number: 58 out of  100\n",
      "Batch number: 59 out of  100\n",
      "Batch number: 60 out of  100\n",
      "Batch number: 61 out of  100\n",
      "Batch number: 62 out of  100\n",
      "Batch number: 63 out of  100\n",
      "Batch number: 64 out of  100\n",
      "Batch number: 65 out of  100\n",
      "Batch number: 66 out of  100\n",
      "Batch number: 67 out of  100\n",
      "Batch number: 68 out of  100\n",
      "Batch number: 69 out of  100\n",
      "Batch number: 70 out of  100\n",
      "Batch number: 71 out of  100\n",
      "Batch number: 72 out of  100\n",
      "Batch number: 73 out of  100\n",
      "Batch number: 74 out of  100\n",
      "Batch number: 75 out of  100\n",
      "Batch number: 76 out of  100\n",
      "Batch number: 77 out of  100\n",
      "Batch number: 78 out of  100\n",
      "Batch number: 79 out of  100\n",
      "Batch number: 80 out of  100\n",
      "Batch number: 81 out of  100\n",
      "Batch number: 82 out of  100\n",
      "Batch number: 83 out of  100\n",
      "Batch number: 84 out of  100\n",
      "Batch number: 85 out of  100\n",
      "Batch number: 86 out of  100\n",
      "Batch number: 87 out of  100\n",
      "Batch number: 88 out of  100\n",
      "Batch number: 89 out of  100\n",
      "Batch number: 90 out of  100\n",
      "Batch number: 91 out of  100\n",
      "Batch number: 92 out of  100\n",
      "Batch number: 93 out of  100\n",
      "Batch number: 94 out of  100\n",
      "Batch number: 95 out of  100\n",
      "Batch number: 96 out of  100\n",
      "Batch number: 97 out of  100\n",
      "Batch number: 98 out of  100\n",
      "Batch number: 99 out of  100\n",
      "Batch number: 100 out of  100\n",
      "Batch number: 101 out of  100\n",
      "CPU times: user 51min 37s, sys: 2min 13s, total: 53min 51s\n",
      "Wall time: 5min 7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_502</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alphabets selfdrive division</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>-0.006629</td>\n",
       "      <td>-0.044545</td>\n",
       "      <td>0.018462</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.055933</td>\n",
       "      <td>-0.026835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045140</td>\n",
       "      <td>0.064447</td>\n",
       "      <td>-0.019442</td>\n",
       "      <td>-0.030245</td>\n",
       "      <td>-0.033326</td>\n",
       "      <td>0.034933</td>\n",
       "      <td>0.047587</td>\n",
       "      <td>-0.008539</td>\n",
       "      <td>-0.012179</td>\n",
       "      <td>0.028646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social media website</td>\n",
       "      <td>-0.049114</td>\n",
       "      <td>0.059202</td>\n",
       "      <td>-0.006767</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.008514</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>-0.089742</td>\n",
       "      <td>0.018635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072986</td>\n",
       "      <td>0.014369</td>\n",
       "      <td>-0.049359</td>\n",
       "      <td>-0.097667</td>\n",
       "      <td>0.053639</td>\n",
       "      <td>-0.071498</td>\n",
       "      <td>0.044204</td>\n",
       "      <td>0.016159</td>\n",
       "      <td>0.052809</td>\n",
       "      <td>0.040297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mr trumps announcement</td>\n",
       "      <td>-0.016972</td>\n",
       "      <td>-0.048336</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>-0.046476</td>\n",
       "      <td>0.024367</td>\n",
       "      <td>0.052372</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>-0.032017</td>\n",
       "      <td>0.060843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022813</td>\n",
       "      <td>-0.007847</td>\n",
       "      <td>0.107963</td>\n",
       "      <td>0.033170</td>\n",
       "      <td>0.039232</td>\n",
       "      <td>0.013165</td>\n",
       "      <td>-0.026472</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>-0.026721</td>\n",
       "      <td>0.019282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mike finn wolfhard</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>-0.051756</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.062292</td>\n",
       "      <td>-0.027771</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>0.114346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020548</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>-0.071563</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>0.017611</td>\n",
       "      <td>0.009212</td>\n",
       "      <td>-0.027160</td>\n",
       "      <td>0.021744</td>\n",
       "      <td>0.042835</td>\n",
       "      <td>0.046351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skincare business</td>\n",
       "      <td>-0.014702</td>\n",
       "      <td>-0.026797</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.048319</td>\n",
       "      <td>0.030793</td>\n",
       "      <td>0.040447</td>\n",
       "      <td>0.010290</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023250</td>\n",
       "      <td>0.043505</td>\n",
       "      <td>-0.064382</td>\n",
       "      <td>-0.010494</td>\n",
       "      <td>0.053259</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>-0.021420</td>\n",
       "      <td>-0.036845</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>-0.013873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           word     emb_0     emb_1     emb_2     emb_3  \\\n",
       "0  alphabets selfdrive division  0.003092 -0.006629 -0.044545  0.018462   \n",
       "1          social media website -0.049114  0.059202 -0.006767  0.001231   \n",
       "2        mr trumps announcement -0.016972 -0.048336  0.001735 -0.046476   \n",
       "3            mike finn wolfhard  0.006570 -0.051756 -0.002061  0.062292   \n",
       "4             skincare business -0.014702 -0.026797  0.013542  0.017857   \n",
       "\n",
       "      emb_4     emb_5     emb_6     emb_7     emb_8  ...   emb_502   emb_503  \\\n",
       "0  0.001877  0.004101  0.034085  0.055933 -0.026835  ...  0.045140  0.064447   \n",
       "1  0.001985  0.008514  0.003320 -0.089742  0.018635  ... -0.072986  0.014369   \n",
       "2  0.024367  0.052372  0.003326 -0.032017  0.060843  ... -0.022813 -0.007847   \n",
       "3 -0.027771  0.000666  0.002715  0.020261  0.114346  ... -0.020548 -0.002933   \n",
       "4  0.048319  0.030793  0.040447  0.010290  0.007345  ... -0.023250  0.043505   \n",
       "\n",
       "    emb_504   emb_505   emb_506   emb_507   emb_508   emb_509   emb_510  \\\n",
       "0 -0.019442 -0.030245 -0.033326  0.034933  0.047587 -0.008539 -0.012179   \n",
       "1 -0.049359 -0.097667  0.053639 -0.071498  0.044204  0.016159  0.052809   \n",
       "2  0.107963  0.033170  0.039232  0.013165 -0.026472  0.058270 -0.026721   \n",
       "3 -0.071563 -0.012376  0.017611  0.009212 -0.027160  0.021744  0.042835   \n",
       "4 -0.064382 -0.010494  0.053259 -0.003029 -0.021420 -0.036845  0.008559   \n",
       "\n",
       "    emb_511  \n",
       "0  0.028646  \n",
       "1  0.040297  \n",
       "2  0.019282  \n",
       "3  0.046351  \n",
       "4 -0.013873  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#creating word2vec matrix\n",
    "df_w2v = get_word_embeddings(df_words, column = \"word\", N_batches=100)\n",
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_502</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alphabets selfdrive division</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>-0.006629</td>\n",
       "      <td>-0.044545</td>\n",
       "      <td>0.018462</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.055933</td>\n",
       "      <td>-0.026835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045140</td>\n",
       "      <td>0.064447</td>\n",
       "      <td>-0.019442</td>\n",
       "      <td>-0.030245</td>\n",
       "      <td>-0.033326</td>\n",
       "      <td>0.034933</td>\n",
       "      <td>0.047587</td>\n",
       "      <td>-0.008539</td>\n",
       "      <td>-0.012179</td>\n",
       "      <td>0.028646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>london cnn businessjust</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>-0.052576</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.043876</td>\n",
       "      <td>0.041327</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>-0.020806</td>\n",
       "      <td>-0.039280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007992</td>\n",
       "      <td>-0.026983</td>\n",
       "      <td>0.120966</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>-0.054596</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.067616</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>0.015005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>evening weekend hour</td>\n",
       "      <td>0.025990</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.039863</td>\n",
       "      <td>0.023469</td>\n",
       "      <td>0.009234</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>-0.008845</td>\n",
       "      <td>-0.015654</td>\n",
       "      <td>-0.036591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082280</td>\n",
       "      <td>-0.038906</td>\n",
       "      <td>0.020474</td>\n",
       "      <td>-0.056085</td>\n",
       "      <td>0.097296</td>\n",
       "      <td>-0.023613</td>\n",
       "      <td>-0.005806</td>\n",
       "      <td>-0.028088</td>\n",
       "      <td>0.093063</td>\n",
       "      <td>-0.053632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              word     emb_0     emb_1     emb_2     emb_3  \\\n",
       "0     alphabets selfdrive division  0.003092 -0.006629 -0.044545  0.018462   \n",
       "3246       london cnn businessjust  0.053961 -0.052576  0.008602  0.043876   \n",
       "2299          evening weekend hour  0.025990  0.001921  0.039863  0.023469   \n",
       "\n",
       "         emb_4     emb_5     emb_6     emb_7     emb_8  ...   emb_502  \\\n",
       "0     0.001877  0.004101  0.034085  0.055933 -0.026835  ...  0.045140   \n",
       "3246  0.041327  0.028829  0.028812 -0.020806 -0.039280  ...  0.007992   \n",
       "2299  0.009234  0.007211 -0.008845 -0.015654 -0.036591  ...  0.082280   \n",
       "\n",
       "       emb_503   emb_504   emb_505   emb_506   emb_507   emb_508   emb_509  \\\n",
       "0     0.064447 -0.019442 -0.030245 -0.033326  0.034933  0.047587 -0.008539   \n",
       "3246 -0.026983  0.120966  0.000980 -0.054596  0.013822  0.003440  0.067616   \n",
       "2299 -0.038906  0.020474 -0.056085  0.097296 -0.023613 -0.005806 -0.028088   \n",
       "\n",
       "       emb_510   emb_511  \n",
       "0    -0.012179  0.028646  \n",
       "3246 -0.001991  0.015005  \n",
       "2299  0.093063 -0.053632  \n",
       "\n",
       "[3 rows x 513 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v.iloc[::150001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction \n",
    "UMAP https://github.com/lmcinnes/umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419327, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"emb_\" + str(i) for i in range(512)]\n",
    "embeddings = df_w2v[columns].values\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55min 11s, sys: 1min 27s, total: 56min 38s\n",
      "Wall time: 6min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(419327, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)\n",
    "umap_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./transition_files/umap_embeddings.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(umap_embeddings, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419327,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster labels\n",
    "labels = hdbscan_cluster.labels_\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 4329\n"
     ]
    }
   ],
   "source": [
    "#number of clusters (key-words/phrases)\n",
    "print(\"Number of clusters:\",labels.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./transition_files/hdbscan_cluster.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(hdbscan_cluster, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get cluster label as most frequent word/phrase of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alphabets selfdrive division</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social media website</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mr trumps announcement</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mike finn wolfhard</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skincare business</td>\n",
       "      <td>4030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           word  cluster_number\n",
       "0  alphabets selfdrive division             228\n",
       "1          social media website             423\n",
       "2        mr trumps announcement              -1\n",
       "3            mike finn wolfhard              -1\n",
       "4             skincare business            4030"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_w2v[['word']].copy()\n",
    "df_tmp['cluster_number'] = labels\n",
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092379, 3)\n"
     ]
    }
   ],
   "source": [
    "df_all_words = pd.DataFrame({'word': list(all_NPs + all_Vs),\n",
    "                             'cluster_label': list(all_NPs + all_Vs)\n",
    "                            })\n",
    "df_all_words = df_all_words.merge(df_tmp, on='word', how='inner')\n",
    "print(df_all_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>word_frequency</th>\n",
       "      <th>word_max_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222220</th>\n",
       "      <td>detroit</td>\n",
       "      <td>detroit</td>\n",
       "      <td>3572</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444440</th>\n",
       "      <td>president trump</td>\n",
       "      <td>president trump</td>\n",
       "      <td>3234</td>\n",
       "      <td>276</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666660</th>\n",
       "      <td>hostility</td>\n",
       "      <td>hostility</td>\n",
       "      <td>724</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888880</th>\n",
       "      <td>ink</td>\n",
       "      <td>ink</td>\n",
       "      <td>1037</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111100</th>\n",
       "      <td>school bus</td>\n",
       "      <td>school bus</td>\n",
       "      <td>3267</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333320</th>\n",
       "      <td>private new space company</td>\n",
       "      <td>private new space company</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555540</th>\n",
       "      <td>pterodactyl</td>\n",
       "      <td>pterodactyl</td>\n",
       "      <td>1880</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777760</th>\n",
       "      <td>needed</td>\n",
       "      <td>needed</td>\n",
       "      <td>1912</td>\n",
       "      <td>812</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999980</th>\n",
       "      <td>monitors</td>\n",
       "      <td>monitors</td>\n",
       "      <td>2323</td>\n",
       "      <td>40</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              word              cluster_label  cluster_number  \\\n",
       "0                             rise                       rise              -1   \n",
       "222220                     detroit                    detroit            3572   \n",
       "444440             president trump            president trump            3234   \n",
       "666660                   hostility                  hostility             724   \n",
       "888880                         ink                        ink            1037   \n",
       "1111100                 school bus                 school bus            3267   \n",
       "1333320  private new space company  private new space company              -1   \n",
       "1555540                pterodactyl                pterodactyl            1880   \n",
       "1777760                     needed                     needed            1912   \n",
       "1999980                   monitors                   monitors            2323   \n",
       "\n",
       "         word_frequency  word_max_frequency  \n",
       "0                   795                9553  \n",
       "222220               85                  85  \n",
       "444440              276                 850  \n",
       "666660               20                  20  \n",
       "888880               29                  29  \n",
       "1111100              23                 130  \n",
       "1333320               1                9553  \n",
       "1555540               1                  41  \n",
       "1777760             812                2756  \n",
       "1999980              40                 444  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_words['word_frequency'] = df_all_words.groupby(['cluster_number', \n",
    "                                                       'cluster_label'])['word'].transform(\"count\")\n",
    "df_all_words['word_max_frequency'] = df_all_words.groupby(['cluster_number'])['word_frequency'].transform(\"max\")\n",
    "df_all_words.iloc[::222220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>20%</th>\n",
       "      <th>30%</th>\n",
       "      <th>40%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cluster_number</th>\n",
       "      <td>2092379.0</td>\n",
       "      <td>1524.389263</td>\n",
       "      <td>1515.839422</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>2933.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>4318.0</td>\n",
       "      <td>4328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_frequency</th>\n",
       "      <td>2092379.0</td>\n",
       "      <td>840.679864</td>\n",
       "      <td>2361.341249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>3290.0</td>\n",
       "      <td>10360.0</td>\n",
       "      <td>20130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_max_frequency</th>\n",
       "      <td>2092379.0</td>\n",
       "      <td>4265.378710</td>\n",
       "      <td>4717.139590</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>9553.0</td>\n",
       "      <td>9553.0</td>\n",
       "      <td>20130.0</td>\n",
       "      <td>20130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count         mean          std  min   1%   10%  \\\n",
       "cluster_number      2092379.0  1524.389263  1515.839422 -1.0 -1.0  -1.0   \n",
       "word_frequency      2092379.0   840.679864  2361.341249  1.0  1.0   1.0   \n",
       "word_max_frequency  2092379.0  4265.378710  4717.139590  1.0  8.0  62.0   \n",
       "\n",
       "                      20%    30%    40%     50%     75%     95%      99%  \\\n",
       "cluster_number       -1.0   -1.0  421.0  1186.0  2933.0  4210.0   4318.0   \n",
       "word_frequency        3.0   13.0   46.0   122.0   694.0  3290.0  10360.0   \n",
       "word_max_frequency  165.0  366.0  655.0  1245.0  9553.0  9553.0  20130.0   \n",
       "\n",
       "                        max  \n",
       "cluster_number       4328.0  \n",
       "word_frequency      20130.0  \n",
       "word_max_frequency  20130.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_all_words.describe(percentiles=[0.01,0.1,0.20,0.3,0.4,0.5,0.75,0.95,0.99])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749438 1342941\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>word_frequency</th>\n",
       "      <th>word_max_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word cluster_label  cluster_number  word_frequency  word_max_frequency\n",
       "0  rise          rise              -1             795                9553\n",
       "1  rise          rise              -1             795                9553\n",
       "2  rise          rise              -1             795                9553\n",
       "3  rise          rise              -1             795                9553\n",
       "4  rise          rise              -1             795                9553"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp_noise = pd.DataFrame(df_all_words[df_all_words['cluster_number'] == -1])\n",
    "df_tmp_other = df_all_words[df_all_words['cluster_number'] != -1]\n",
    "print(len(df_tmp_noise),len(df_tmp_other))\n",
    "df_tmp_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4329, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tencent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>diaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>almond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>3000</td>\n",
       "      <td>good day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>4000</td>\n",
       "      <td>minnesota</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cluster_number cluster_label\n",
       "0                  0       tencent\n",
       "1000            1000        diaper\n",
       "2000            2000        almond\n",
       "3000            3000      good day\n",
       "4000            4000     minnesota"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_tmp_other[df_tmp_other['word_max_frequency'] == df_tmp_other['word_frequency']]\n",
    "df_tmp = df_tmp.groupby('cluster_number')['cluster_label'].last().reset_index()\n",
    "print(df_tmp.shape)\n",
    "df_tmp.iloc[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092379, 2)\n",
      "(419327, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>222222</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>rise</td>\n",
       "      <td>um hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_number</th>\n",
       "      <td>-1</td>\n",
       "      <td>2655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_label</th>\n",
       "      <td>noise</td>\n",
       "      <td>hand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        222222\n",
       "word             rise  um hand\n",
       "cluster_number     -1     2655\n",
       "cluster_label   noise     hand"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_clusters = df_all_words[['word', 'cluster_number']]\n",
    "print(df_word_clusters.shape)\n",
    "\n",
    "df_word_clusters = df_word_clusters.drop_duplicates()\n",
    "df_word_clusters = df_word_clusters.merge(df_tmp, on='cluster_number', how='left')\n",
    "df_word_clusters['cluster_label'] = df_word_clusters['cluster_label'].fillna(\"noise\")\n",
    "\n",
    "print(df_word_clusters.shape)\n",
    "df_word_clusters.iloc[::222222].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      4330.000000\n",
       "mean         96.842263\n",
       "std        2538.397253\n",
       "min          15.000000\n",
       "25%          23.000000\n",
       "50%          36.000000\n",
       "75%          65.000000\n",
       "max      166821.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_word_clusters.groupby('cluster_label')['word'].count()\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_label\n",
       "zika          109\n",
       "zombie         36\n",
       "zoo            27\n",
       "zoom          150\n",
       "zuckerberg     88\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24011</th>\n",
       "      <td>zombie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24016</th>\n",
       "      <td>corporate zombie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24022</th>\n",
       "      <td>zombie firmscompanie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76572</th>\n",
       "      <td>zombie outbreak</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176015</th>\n",
       "      <td>zombie virus</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182615</th>\n",
       "      <td>zombie star</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193144</th>\n",
       "      <td>zombie infectionsincluding prevention</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193150</th>\n",
       "      <td>zombie paper</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193152</th>\n",
       "      <td>actual fictional zombie literature</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199134</th>\n",
       "      <td>zombie army</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201778</th>\n",
       "      <td>brief zombie awakening</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215369</th>\n",
       "      <td>zombies</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215379</th>\n",
       "      <td>zombies original concept</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228438</th>\n",
       "      <td>giant zombie attack dog</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231289</th>\n",
       "      <td>zombie flick</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231761</th>\n",
       "      <td>zombie politician</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232715</th>\n",
       "      <td>sweet funny lighthearted zombie movie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236129</th>\n",
       "      <td>zombie basket</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247919</th>\n",
       "      <td>zombie internet</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253379</th>\n",
       "      <td>zombie food</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266976</th>\n",
       "      <td>socially minded zombie movie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269679</th>\n",
       "      <td>single zombie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275683</th>\n",
       "      <td>zombie connecteddevice army</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293430</th>\n",
       "      <td>zombie hunter</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313183</th>\n",
       "      <td>zombie movie army</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314835</th>\n",
       "      <td>zombie component</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319086</th>\n",
       "      <td>zombie rumor</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321986</th>\n",
       "      <td>zombie movie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322312</th>\n",
       "      <td>floating zombie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327172</th>\n",
       "      <td>zombie thriller</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327276</th>\n",
       "      <td>tired zombie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352730</th>\n",
       "      <td>omnipresent zombie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354781</th>\n",
       "      <td>zombie text message</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355214</th>\n",
       "      <td>zombie bill</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368723</th>\n",
       "      <td>zombie chemical</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382058</th>\n",
       "      <td>natures microscopic zombie</td>\n",
       "      <td>313</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         word  cluster_number cluster_label\n",
       "24011                                  zombie             313        zombie\n",
       "24016                        corporate zombie             313        zombie\n",
       "24022                    zombie firmscompanie             313        zombie\n",
       "76572                         zombie outbreak             313        zombie\n",
       "176015                           zombie virus             313        zombie\n",
       "182615                            zombie star             313        zombie\n",
       "193144  zombie infectionsincluding prevention             313        zombie\n",
       "193150                           zombie paper             313        zombie\n",
       "193152     actual fictional zombie literature             313        zombie\n",
       "199134                            zombie army             313        zombie\n",
       "201778                 brief zombie awakening             313        zombie\n",
       "215369                                zombies             313        zombie\n",
       "215379               zombies original concept             313        zombie\n",
       "228438                giant zombie attack dog             313        zombie\n",
       "231289                           zombie flick             313        zombie\n",
       "231761                      zombie politician             313        zombie\n",
       "232715  sweet funny lighthearted zombie movie             313        zombie\n",
       "236129                          zombie basket             313        zombie\n",
       "247919                        zombie internet             313        zombie\n",
       "253379                            zombie food             313        zombie\n",
       "266976           socially minded zombie movie             313        zombie\n",
       "269679                          single zombie             313        zombie\n",
       "275683            zombie connecteddevice army             313        zombie\n",
       "293430                          zombie hunter             313        zombie\n",
       "313183                      zombie movie army             313        zombie\n",
       "314835                       zombie component             313        zombie\n",
       "319086                           zombie rumor             313        zombie\n",
       "321986                           zombie movie             313        zombie\n",
       "322312                        floating zombie             313        zombie\n",
       "327172                        zombie thriller             313        zombie\n",
       "327276                           tired zombie             313        zombie\n",
       "352730                     omnipresent zombie             313        zombie\n",
       "354781                    zombie text message             313        zombie\n",
       "355214                            zombie bill             313        zombie\n",
       "368723                        zombie chemical             313        zombie\n",
       "382058             natures microscopic zombie             313        zombie"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_clusters[df_word_clusters['cluster_label'] == \"zombie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_clusters.to_csv('./transition_files/word_cluster_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Replace text words with their cluster names (KeyWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
       "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
       "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
       "       'group_level_1', 'group_level_2', 'group_level_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['all_words'] = df_train['list_of_verb_lemmas'] + df_train['noun_phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster_label_dict =dict(zip(df_word_clusters['word'], df_word_clusters['cluster_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_key_words</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[making, led, noise, travel, noise, upgrade, travel, noise, noise, noise, noise, noise, economic, noise, noise, noise, noise, surge, number, peopl...</td>\n",
       "      <td>[merging, led, wanting, travel, granted, upgrade, travel, apply, submit, streamline, scra, rise, big emerging economy, china, india, steady march,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[noise, noise, noise, gain, raised, noise, prevent, noise, noise, shift, noise, excited, noise, noise, noise, promise, noise, letting, noise, tryi...</td>\n",
       "      <td>[rided, embracing, insists, gain, strengthen, improve, deterred, seeking, intends, shift, domiciled, rejoiced, saved, paid, outraged, promised, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[noise, end, noise, trying, lift, noise, reverse, noise, help, understand, upgrade, strike, wish, save, noise, trying, escape, noise, noise, pulle...</td>\n",
       "      <td>[aised, ended, celebrate, tried, lift, forced, reverse, cut, help, understand, upgrade, strike, wish, save, spend, try, escape, slashing, encourag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[race, reserved, noise, announced, control, demand, noise, noise, noise, noise, moving, upgrade, noise, announced, making, goal, noise, ad, cruise...</td>\n",
       "      <td>[race, booked, improve, announced, control, demand, peaking, piling, based, got, moving, upgrade, increase, announced, establish, aimed, based, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[noise, noise, noise, noise, noise, suggest, judgment, betting, expect, upgrade, noise, noise, noise, noise, buy, noise, believes, noise, noise, i...</td>\n",
       "      <td>[tart, caught, proved, reflected, like, suggest, judged, betting, expect, upgrade, weakens, having, pushed, tighten, buy, priced, doubt, tighten, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           all_key_words  \\\n",
       "0  [making, led, noise, travel, noise, upgrade, travel, noise, noise, noise, noise, noise, economic, noise, noise, noise, noise, surge, number, peopl...   \n",
       "1  [noise, noise, noise, gain, raised, noise, prevent, noise, noise, shift, noise, excited, noise, noise, noise, promise, noise, letting, noise, tryi...   \n",
       "2  [noise, end, noise, trying, lift, noise, reverse, noise, help, understand, upgrade, strike, wish, save, noise, trying, escape, noise, noise, pulle...   \n",
       "3  [race, reserved, noise, announced, control, demand, noise, noise, noise, noise, moving, upgrade, noise, announced, making, goal, noise, ad, cruise...   \n",
       "4  [noise, noise, noise, noise, noise, suggest, judgment, betting, expect, upgrade, noise, noise, noise, noise, buy, noise, believes, noise, noise, i...   \n",
       "\n",
       "                                                                                                                                               all_words  \n",
       "0  [merging, led, wanting, travel, granted, upgrade, travel, apply, submit, streamline, scra, rise, big emerging economy, china, india, steady march,...  \n",
       "1  [rided, embracing, insists, gain, strengthen, improve, deterred, seeking, intends, shift, domiciled, rejoiced, saved, paid, outraged, promised, im...  \n",
       "2  [aised, ended, celebrate, tried, lift, forced, reverse, cut, help, understand, upgrade, strike, wish, save, spend, try, escape, slashing, encourag...  \n",
       "3  [race, booked, improve, announced, control, demand, peaking, piling, based, got, moving, upgrade, increase, announced, establish, aimed, based, ad...  \n",
       "4  [tart, caught, proved, reflected, like, suggest, judged, betting, expect, upgrade, weakens, having, pushed, tighten, buy, priced, doubt, tighten, ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['all_key_words'] = df_train['all_words'].apply(lambda Wlist: \n",
    "                                                        [word_cluster_label_dict[w] for w in  Wlist\n",
    "                                                                                    if w in word_cluster_label_dict\n",
    "                                                        ])\n",
    "df_train[['all_key_words', 'all_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./transition_files/df_train_for_LDA.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(df_train, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GLG - 3_step - Assigning Text Groups.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.45px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
