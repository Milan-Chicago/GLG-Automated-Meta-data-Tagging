{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea:\n",
    "Our solution: LDA + keywords from clusters of BERT based embeddings of noun phrases and verbs :\n",
    "- Each noun phrase and verb in the texts is  transformed to embedding vector using Universal Sentence Encoder (transformer based on BERT)\n",
    "- Embedding vectors from (a) are clustered (HDBSCAN + UNET)\n",
    "- Words/phrases with embedding vectors closest to the centers of resulting clusters form key word/phrase\n",
    "- Each text in the training sample is converted to collection of key-phrases by replacing its noun phrases and verbs with keyword/phrases and deleting other words\n",
    "- LDA is performed on the transformed texts\n",
    "\n",
    "\n",
    "**Reference:**<br>\n",
    "- Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-CÃ©spedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. **Universal Sentence Encoder.** *arXiv:1803.11175, 2018.*\n",
    "- McInnes, L, Healy, J, **UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction**, *ArXiv e-prints 1802.03426, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clNUUp3MUO2t"
   },
   "source": [
    "# Load data and python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1622131163361,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "7dYQIbH6UO2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "module https://tfhub.dev/google/universal-sentence-encoder-large/5 loaded\n"
     ]
    }
   ],
   "source": [
    "# data processing libraries\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# display wider columns in pandas data frames where necessary\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "#Load the Universal Sentence Encoder's TF Hub module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (33982, 16)\n",
      "df_train.shape: Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
      "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
      "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
      "       'group_level_1', 'group_level_2', 'group_level_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data/train_grouped.tsv\", sep=\"\\t\")\n",
    "print(\"df_train.shape:\", df_train.shape)\n",
    "print(\"df_train.shape:\",df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bxk10XYUqNp"
   },
   "source": [
    "# Getting text clusters through sentence embedding comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1622131190534,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "MsgF9abaX2Bn"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(df_data, column = \"word\", N_batches=1):\n",
    "    #split data into N batches\n",
    "    N = N_batches\n",
    "\n",
    "    part = int(len(df_data)/N)\n",
    "    print(N, \"batches with\", part + 1, column + \"s each\")\n",
    "\n",
    "    #get embeddings for each N words\n",
    "    index = 0\n",
    "    batch_num = 0\n",
    "    list_dfs = []\n",
    "\n",
    "    while index < len(df_data): \n",
    "        df_tmp = df_data.iloc[index : index + part].copy()\n",
    "        df_tmp = df_tmp.reset_index(drop=True)\n",
    "        print (\"Batch number:\", batch_num + 1, \"out of \", N)\n",
    "\n",
    "        df_batch_embeddings = pd.DataFrame(get_embeddings(list(df_tmp[column])).numpy())\n",
    "\n",
    "        num_embeddings = df_batch_embeddings.shape[1]\n",
    "        columns = [\"emb_\" + str(i) for i in range(512)]\n",
    "        df_tmp[columns] = df_batch_embeddings\n",
    "\n",
    "        list_dfs.append(df_tmp)\n",
    "        batch_num = batch_num + 1\n",
    "        index = index + part\n",
    "\n",
    "    #concatinate batches into single dataset\n",
    "    df_emb = pd.concat(list_dfs)\n",
    "\n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [rise, big emerging economy, china, india, steady march, globalisation, surge, number, people, business, tourism, result, demand, visa, unpreceden...\n",
       "1    [pfizer, commitment, corporate social responsibility csr, drugs giant talk, responsibility, society, world, access, product, work, ngos, global he...\n",
       "2    [week, federal reserve, interest rate, time, year, world, central bank, rate, recent year, long spell, course, chart, outcome, americas rate rise,...\n",
       "3    [cruise line, wave, year, nearly, holiday, sea, result, december 18th carnival, worlds largest operator, global market, fullyear earning, demand, ...\n",
       "4    [investors, calendar year, buoyant mood, unexpected event, consensus, respect, view, investor, market price, column, potential surprise, definitio...\n",
       "Name: noun_phrases, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['noun_phrases'] = df_train['noun_phrases'].str[2:-2]\n",
    "df_train['noun_phrases'] = df_train['noun_phrases'].str.lower().str.split(\"', '\")\n",
    "df_train['noun_phrases'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['rise', 'big emerging economy', 'china', 'india', 'steady march'], 1417049)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_NPs = list(df_train['noun_phrases'])\n",
    "all_NPs = [np for l in all_NPs for np in l if len(np)>0]\n",
    "all_NPs[:5], len(all_NPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[emerging, led, wanting, travel, granted, Upgrade, travel, apply, submit, streamline, scrap]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['list_of_verb_lemmas'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                               [merging, led, wanting, travel, granted, upgrade, travel, apply, submit, streamline, scra]\n",
       "1    [rided, embracing, insists, gain, strengthen, improve, deterred, seeking, intends, shift, domiciled, rejoiced, saved, paid, outraged, promised, im...\n",
       "2    [aised, ended, celebrate, tried, lift, forced, reverse, cut, help, understand, upgrade, strike, wish, save, spend, try, escape, slashing, encourag...\n",
       "3      [race, booked, improve, announced, control, demand, peaking, piling, based, got, moving, upgrade, increase, announced, establish, aimed, based, ad]\n",
       "4    [tart, caught, proved, reflected, like, suggest, judged, betting, expect, upgrade, weakens, having, pushed, tighten, buy, priced, doubt, tighten, ...\n",
       "Name: list_of_verb_lemmas, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['list_of_verb_lemmas'] = df_train['list_of_verb_lemmas'].str[2:-2]\n",
    "df_train['list_of_verb_lemmas'] = df_train['list_of_verb_lemmas'].str.lower().str.split(\", \")\n",
    "df_train['list_of_verb_lemmas'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['merging', 'led', 'wanting', 'travel', 'granted'], 675330)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_Vs = list(df_train['list_of_verb_lemmas'])\n",
    "all_Vs = [v for l in all_Vs for v in l if len(v)>0]\n",
    "all_Vs[:5], len(all_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419327"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words =  list(set(all_NPs + all_Vs))\n",
    "len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_QElB-4UM2z",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english navy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>affordable iphone xr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nsa hacking team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>driverless</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word\n",
       "0          english navy\n",
       "1  affordable iphone xr\n",
       "2                conley\n",
       "3      nsa hacking team\n",
       "4            driverless"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.DataFrame({'word': all_words})\n",
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 batches with 4194 words each\n",
      "Batch number: 1 out of  100\n",
      "Batch number: 2 out of  100\n",
      "Batch number: 3 out of  100\n",
      "Batch number: 4 out of  100\n",
      "Batch number: 5 out of  100\n",
      "Batch number: 6 out of  100\n",
      "Batch number: 7 out of  100\n",
      "Batch number: 8 out of  100\n",
      "Batch number: 9 out of  100\n",
      "Batch number: 10 out of  100\n",
      "Batch number: 11 out of  100\n",
      "Batch number: 12 out of  100\n",
      "Batch number: 13 out of  100\n",
      "Batch number: 14 out of  100\n",
      "Batch number: 15 out of  100\n",
      "Batch number: 16 out of  100\n",
      "Batch number: 17 out of  100\n",
      "Batch number: 18 out of  100\n",
      "Batch number: 19 out of  100\n",
      "Batch number: 20 out of  100\n",
      "Batch number: 21 out of  100\n",
      "Batch number: 22 out of  100\n",
      "Batch number: 23 out of  100\n",
      "Batch number: 24 out of  100\n",
      "Batch number: 25 out of  100\n",
      "Batch number: 26 out of  100\n",
      "Batch number: 27 out of  100\n",
      "Batch number: 28 out of  100\n",
      "Batch number: 29 out of  100\n",
      "Batch number: 30 out of  100\n",
      "Batch number: 31 out of  100\n",
      "Batch number: 32 out of  100\n",
      "Batch number: 33 out of  100\n",
      "Batch number: 34 out of  100\n",
      "Batch number: 35 out of  100\n",
      "Batch number: 36 out of  100\n",
      "Batch number: 37 out of  100\n",
      "Batch number: 38 out of  100\n",
      "Batch number: 39 out of  100\n",
      "Batch number: 40 out of  100\n",
      "Batch number: 41 out of  100\n",
      "Batch number: 42 out of  100\n",
      "Batch number: 43 out of  100\n",
      "Batch number: 44 out of  100\n",
      "Batch number: 45 out of  100\n",
      "Batch number: 46 out of  100\n",
      "Batch number: 47 out of  100\n",
      "Batch number: 48 out of  100\n",
      "Batch number: 49 out of  100\n",
      "Batch number: 50 out of  100\n",
      "Batch number: 51 out of  100\n",
      "Batch number: 52 out of  100\n",
      "Batch number: 53 out of  100\n",
      "Batch number: 54 out of  100\n",
      "Batch number: 55 out of  100\n",
      "Batch number: 56 out of  100\n",
      "Batch number: 57 out of  100\n",
      "Batch number: 58 out of  100\n",
      "Batch number: 59 out of  100\n",
      "Batch number: 60 out of  100\n",
      "Batch number: 61 out of  100\n",
      "Batch number: 62 out of  100\n",
      "Batch number: 63 out of  100\n",
      "Batch number: 64 out of  100\n",
      "Batch number: 65 out of  100\n",
      "Batch number: 66 out of  100\n",
      "Batch number: 67 out of  100\n",
      "Batch number: 68 out of  100\n",
      "Batch number: 69 out of  100\n",
      "Batch number: 70 out of  100\n",
      "Batch number: 71 out of  100\n",
      "Batch number: 72 out of  100\n",
      "Batch number: 73 out of  100\n",
      "Batch number: 74 out of  100\n",
      "Batch number: 75 out of  100\n",
      "Batch number: 76 out of  100\n",
      "Batch number: 77 out of  100\n",
      "Batch number: 78 out of  100\n",
      "Batch number: 79 out of  100\n",
      "Batch number: 80 out of  100\n",
      "Batch number: 81 out of  100\n",
      "Batch number: 82 out of  100\n",
      "Batch number: 83 out of  100\n",
      "Batch number: 84 out of  100\n",
      "Batch number: 85 out of  100\n",
      "Batch number: 86 out of  100\n",
      "Batch number: 87 out of  100\n",
      "Batch number: 88 out of  100\n",
      "Batch number: 89 out of  100\n",
      "Batch number: 90 out of  100\n",
      "Batch number: 91 out of  100\n",
      "Batch number: 92 out of  100\n",
      "Batch number: 93 out of  100\n",
      "Batch number: 94 out of  100\n",
      "Batch number: 95 out of  100\n",
      "Batch number: 96 out of  100\n",
      "Batch number: 97 out of  100\n",
      "Batch number: 98 out of  100\n",
      "Batch number: 99 out of  100\n",
      "Batch number: 100 out of  100\n",
      "Batch number: 101 out of  100\n",
      "CPU times: user 50min 27s, sys: 2min 19s, total: 52min 46s\n",
      "Wall time: 5min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_502</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english navy</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>0.056447</td>\n",
       "      <td>-0.013358</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.050376</td>\n",
       "      <td>-0.137254</td>\n",
       "      <td>-0.014414</td>\n",
       "      <td>-0.051146</td>\n",
       "      <td>0.013260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053418</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.104392</td>\n",
       "      <td>-0.024585</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>0.011937</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>-0.004595</td>\n",
       "      <td>0.003696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>affordable iphone xr</td>\n",
       "      <td>-0.017070</td>\n",
       "      <td>0.066874</td>\n",
       "      <td>-0.045078</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>0.035852</td>\n",
       "      <td>-0.063014</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.120475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025386</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>-0.035876</td>\n",
       "      <td>-0.007965</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>-0.041157</td>\n",
       "      <td>0.025264</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.006670</td>\n",
       "      <td>-0.018801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conley</td>\n",
       "      <td>-0.086235</td>\n",
       "      <td>0.052709</td>\n",
       "      <td>-0.020092</td>\n",
       "      <td>-0.017369</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>-0.070125</td>\n",
       "      <td>0.032130</td>\n",
       "      <td>-0.012201</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054062</td>\n",
       "      <td>-0.049556</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.031734</td>\n",
       "      <td>0.021291</td>\n",
       "      <td>-0.007574</td>\n",
       "      <td>0.028390</td>\n",
       "      <td>-0.017262</td>\n",
       "      <td>0.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nsa hacking team</td>\n",
       "      <td>-0.007682</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.066631</td>\n",
       "      <td>0.053021</td>\n",
       "      <td>-0.045030</td>\n",
       "      <td>0.017534</td>\n",
       "      <td>-0.015002</td>\n",
       "      <td>0.021589</td>\n",
       "      <td>-0.010603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016597</td>\n",
       "      <td>0.027437</td>\n",
       "      <td>0.081878</td>\n",
       "      <td>-0.057940</td>\n",
       "      <td>-0.021009</td>\n",
       "      <td>0.067911</td>\n",
       "      <td>-0.003568</td>\n",
       "      <td>0.034031</td>\n",
       "      <td>0.059068</td>\n",
       "      <td>0.088520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>driverless</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.045812</td>\n",
       "      <td>0.014513</td>\n",
       "      <td>0.029443</td>\n",
       "      <td>-0.058470</td>\n",
       "      <td>0.045797</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>-0.040082</td>\n",
       "      <td>0.045964</td>\n",
       "      <td>-0.007353</td>\n",
       "      <td>-0.003374</td>\n",
       "      <td>0.059233</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.019076</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.031821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word     emb_0     emb_1     emb_2     emb_3     emb_4  \\\n",
       "0          english navy  0.019252  0.056447 -0.013358  0.032645  0.050376   \n",
       "1  affordable iphone xr -0.017070  0.066874 -0.045078 -0.001512  0.035852   \n",
       "2                conley -0.086235  0.052709 -0.020092 -0.017369  0.010625   \n",
       "3      nsa hacking team -0.007682  0.017487  0.066631  0.053021 -0.045030   \n",
       "4            driverless  0.000395  0.009780  0.045812  0.014513  0.029443   \n",
       "\n",
       "      emb_5     emb_6     emb_7     emb_8  ...   emb_502   emb_503   emb_504  \\\n",
       "0 -0.137254 -0.014414 -0.051146  0.013260  ... -0.053418  0.002012  0.104392   \n",
       "1 -0.063014  0.004250  0.021522  0.120475  ... -0.025386  0.000343 -0.035876   \n",
       "2 -0.070125  0.032130 -0.012201  0.005054  ... -0.054062 -0.049556  0.016414   \n",
       "3  0.017534 -0.015002  0.021589 -0.010603  ... -0.016597  0.027437  0.081878   \n",
       "4 -0.058470  0.045797  0.006264 -0.000040  ...  0.015909 -0.040082  0.045964   \n",
       "\n",
       "    emb_505   emb_506   emb_507   emb_508   emb_509   emb_510   emb_511  \n",
       "0 -0.024585 -0.007928  0.035448  0.011937  0.013509 -0.004595  0.003696  \n",
       "1 -0.007965  0.030047 -0.041157  0.025264 -0.024750 -0.006670 -0.018801  \n",
       "2  0.009363  0.031734  0.021291 -0.007574  0.028390 -0.017262  0.011601  \n",
       "3 -0.057940 -0.021009  0.067911 -0.003568  0.034031  0.059068  0.088520  \n",
       "4 -0.007353 -0.003374  0.059233  0.005722  0.019076  0.002116  0.031821  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#creating word2vec matrix\n",
    "df_w2v = get_word_embeddings(df_words, column = \"word\", N_batches=100)\n",
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_502</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english navy</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>0.056447</td>\n",
       "      <td>-0.013358</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.050376</td>\n",
       "      <td>-0.137254</td>\n",
       "      <td>-0.014414</td>\n",
       "      <td>-0.051146</td>\n",
       "      <td>0.013260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053418</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.104392</td>\n",
       "      <td>-0.024585</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>0.011937</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>-0.004595</td>\n",
       "      <td>0.003696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>insane smartphone growth</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>0.035906</td>\n",
       "      <td>0.041326</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>-0.040255</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.020335</td>\n",
       "      <td>0.113420</td>\n",
       "      <td>0.153041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>-0.036631</td>\n",
       "      <td>-0.006495</td>\n",
       "      <td>0.077364</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>-0.098226</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.060544</td>\n",
       "      <td>0.043139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>visceral</td>\n",
       "      <td>-0.029612</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.036229</td>\n",
       "      <td>-0.062932</td>\n",
       "      <td>-0.003113</td>\n",
       "      <td>0.049451</td>\n",
       "      <td>0.071272</td>\n",
       "      <td>0.055529</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070087</td>\n",
       "      <td>-0.008031</td>\n",
       "      <td>-0.024188</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>-0.071140</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.010009</td>\n",
       "      <td>-0.053891</td>\n",
       "      <td>-0.005071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          word     emb_0     emb_1     emb_2     emb_3  \\\n",
       "0                 english navy  0.019252  0.056447 -0.013358  0.032645   \n",
       "3246  insane smartphone growth  0.027154  0.035906  0.041326  0.001631   \n",
       "2299                  visceral -0.029612  0.000500  0.036229 -0.062932   \n",
       "\n",
       "         emb_4     emb_5     emb_6     emb_7     emb_8  ...   emb_502  \\\n",
       "0     0.050376 -0.137254 -0.014414 -0.051146  0.013260  ... -0.053418   \n",
       "3246 -0.040255  0.019108  0.020335  0.113420  0.153041  ...  0.013288   \n",
       "2299 -0.003113  0.049451  0.071272  0.055529  0.000105  ... -0.070087   \n",
       "\n",
       "       emb_503   emb_504   emb_505   emb_506   emb_507   emb_508   emb_509  \\\n",
       "0     0.002012  0.104392 -0.024585 -0.007928  0.035448  0.011937  0.013509   \n",
       "3246  0.045752 -0.036631 -0.006495  0.077364  0.004067 -0.098226  0.066084   \n",
       "2299 -0.008031 -0.024188  0.002269 -0.071140  0.006097  0.001805  0.010009   \n",
       "\n",
       "       emb_510   emb_511  \n",
       "0    -0.004595  0.003696  \n",
       "3246  0.060544  0.043139  \n",
       "2299 -0.053891 -0.005071  \n",
       "\n",
       "[3 rows x 513 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v.iloc[::150001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction \n",
    "UMAP https://github.com/lmcinnes/umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419327, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"emb_\" + str(i) for i in range(512)]\n",
    "embeddings = df_w2v[columns].values\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58min 18s, sys: 1min 8s, total: 59min 26s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "umap_model = umap.UMAP(n_neighbors=15, \n",
    "                        n_components=5, \n",
    "                        metric='cosine').fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.63 s, sys: 588 ms, total: 2.22 s\n",
      "Wall time: 2.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(419327, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "umap_embeddings = umap_model.transform(embeddings)\n",
    "umap_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./transition_files/umap_model.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(umap_model, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419327,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster labels\n",
    "labels = hdbscan_cluster.labels_\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 4303\n"
     ]
    }
   ],
   "source": [
    "#number of clusters (key-words/phrases)\n",
    "print(\"Number of clusters:\",labels.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./transition_files/hdbscan_cluster.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(hdbscan_cluster, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get cluster label as most frequent word/phrase of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english navy</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>affordable iphone xr</td>\n",
       "      <td>2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conley</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nsa hacking team</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>driverless</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  cluster_number\n",
       "0          english navy             761\n",
       "1  affordable iphone xr            2260\n",
       "2                conley              -1\n",
       "3      nsa hacking team              -1\n",
       "4            driverless              -1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_w2v[['word']].copy()\n",
    "df_tmp['cluster_number'] = labels\n",
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092379, 3)\n"
     ]
    }
   ],
   "source": [
    "df_all_words = pd.DataFrame({'word': list(all_NPs + all_Vs),\n",
    "                             'cluster_label': list(all_NPs + all_Vs)\n",
    "                            })\n",
    "df_all_words = df_all_words.merge(df_tmp, on='word', how='inner')\n",
    "print(df_all_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>word_frequency</th>\n",
       "      <th>word_max_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222220</th>\n",
       "      <td>detroit</td>\n",
       "      <td>detroit</td>\n",
       "      <td>3332</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444440</th>\n",
       "      <td>president trump</td>\n",
       "      <td>president trump</td>\n",
       "      <td>3370</td>\n",
       "      <td>276</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666660</th>\n",
       "      <td>hostility</td>\n",
       "      <td>hostility</td>\n",
       "      <td>1308</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888880</th>\n",
       "      <td>ink</td>\n",
       "      <td>ink</td>\n",
       "      <td>879</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111100</th>\n",
       "      <td>school bus</td>\n",
       "      <td>school bus</td>\n",
       "      <td>3229</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333320</th>\n",
       "      <td>private new space company</td>\n",
       "      <td>private new space company</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555540</th>\n",
       "      <td>pterodactyl</td>\n",
       "      <td>pterodactyl</td>\n",
       "      <td>1564</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777760</th>\n",
       "      <td>needed</td>\n",
       "      <td>needed</td>\n",
       "      <td>3433</td>\n",
       "      <td>812</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999980</th>\n",
       "      <td>monitors</td>\n",
       "      <td>monitors</td>\n",
       "      <td>-1</td>\n",
       "      <td>40</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              word              cluster_label  cluster_number  \\\n",
       "0                             rise                       rise              -1   \n",
       "222220                     detroit                    detroit            3332   \n",
       "444440             president trump            president trump            3370   \n",
       "666660                   hostility                  hostility            1308   \n",
       "888880                         ink                        ink             879   \n",
       "1111100                 school bus                 school bus            3229   \n",
       "1333320  private new space company  private new space company              -1   \n",
       "1555540                pterodactyl                pterodactyl            1564   \n",
       "1777760                     needed                     needed            3433   \n",
       "1999980                   monitors                   monitors              -1   \n",
       "\n",
       "         word_frequency  word_max_frequency  \n",
       "0                   795                9553  \n",
       "222220               85                  85  \n",
       "444440              276                 850  \n",
       "666660               20                  20  \n",
       "888880               29                  29  \n",
       "1111100              23                 130  \n",
       "1333320               1                9553  \n",
       "1555540               1                  41  \n",
       "1777760             812                2756  \n",
       "1999980              40                9553  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_words['word_frequency'] = df_all_words.groupby(['cluster_number', \n",
    "                                                       'cluster_label'])['word'].transform(\"count\")\n",
    "df_all_words['word_max_frequency'] = df_all_words.groupby(['cluster_number'])['word_frequency'].transform(\"max\")\n",
    "df_all_words.iloc[::222220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>10%</th>\n",
       "      <th>20%</th>\n",
       "      <th>30%</th>\n",
       "      <th>40%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cluster_number</th>\n",
       "      <td>2092379.0</td>\n",
       "      <td>1555.766894</td>\n",
       "      <td>1543.933302</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>3047.0</td>\n",
       "      <td>4209.0</td>\n",
       "      <td>4294.0</td>\n",
       "      <td>4302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_frequency</th>\n",
       "      <td>2092379.0</td>\n",
       "      <td>840.679864</td>\n",
       "      <td>2361.341249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>3290.0</td>\n",
       "      <td>10360.0</td>\n",
       "      <td>20130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_max_frequency</th>\n",
       "      <td>2092379.0</td>\n",
       "      <td>4254.398780</td>\n",
       "      <td>4711.107384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>658.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>9553.0</td>\n",
       "      <td>9553.0</td>\n",
       "      <td>20130.0</td>\n",
       "      <td>20130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count         mean          std  min   1%   10%  \\\n",
       "cluster_number      2092379.0  1555.766894  1543.933302 -1.0 -1.0  -1.0   \n",
       "word_frequency      2092379.0   840.679864  2361.341249  1.0  1.0   1.0   \n",
       "word_max_frequency  2092379.0  4254.398780  4711.107384  1.0  8.0  61.0   \n",
       "\n",
       "                      20%    30%    40%     50%     75%     95%      99%  \\\n",
       "cluster_number       -1.0   -1.0  365.0  1205.0  3047.0  4209.0   4294.0   \n",
       "word_frequency        3.0   13.0   46.0   122.0   694.0  3290.0  10360.0   \n",
       "word_max_frequency  165.0  365.0  658.0  1252.0  9553.0  9553.0  20130.0   \n",
       "\n",
       "                        max  \n",
       "cluster_number       4302.0  \n",
       "word_frequency      20130.0  \n",
       "word_max_frequency  20130.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_all_words.describe(percentiles=[0.01,0.1,0.20,0.3,0.4,0.5,0.75,0.95,0.99])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745515 1346864\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>word_frequency</th>\n",
       "      <th>word_max_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rise</td>\n",
       "      <td>rise</td>\n",
       "      <td>-1</td>\n",
       "      <td>795</td>\n",
       "      <td>9553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word cluster_label  cluster_number  word_frequency  word_max_frequency\n",
       "0  rise          rise              -1             795                9553\n",
       "1  rise          rise              -1             795                9553\n",
       "2  rise          rise              -1             795                9553\n",
       "3  rise          rise              -1             795                9553\n",
       "4  rise          rise              -1             795                9553"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp_noise = pd.DataFrame(df_all_words[df_all_words['cluster_number'] == -1])\n",
    "df_tmp_other = df_all_words[df_all_words['cluster_number'] != -1]\n",
    "print(len(df_tmp_noise),len(df_tmp_other))\n",
    "df_tmp_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4303, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gopro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>responsibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>export</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>3000</td>\n",
       "      <td>lack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>4000</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cluster_number   cluster_label\n",
       "0                  0           gopro\n",
       "1000            1000  responsibility\n",
       "2000            2000          export\n",
       "3000            3000            lack\n",
       "4000            4000          object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_tmp_other[df_tmp_other['word_max_frequency'] == df_tmp_other['word_frequency']]\n",
    "df_tmp = df_tmp.groupby('cluster_number')['cluster_label'].last().reset_index()\n",
    "print(df_tmp.shape)\n",
    "df_tmp.iloc[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092379, 2)\n",
      "(419327, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>222222</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>rise</td>\n",
       "      <td>um hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_number</th>\n",
       "      <td>-1</td>\n",
       "      <td>2819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_label</th>\n",
       "      <td>noise</td>\n",
       "      <td>hand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        222222\n",
       "word             rise  um hand\n",
       "cluster_number     -1     2819\n",
       "cluster_label   noise     hand"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_clusters = df_all_words[['word', 'cluster_number']]\n",
    "print(df_word_clusters.shape)\n",
    "\n",
    "df_word_clusters = df_word_clusters.drop_duplicates()\n",
    "df_word_clusters = df_word_clusters.merge(df_tmp, on='cluster_number', how='left')\n",
    "df_word_clusters['cluster_label'] = df_word_clusters['cluster_label'].fillna(\"noise\")\n",
    "\n",
    "print(df_word_clusters.shape)\n",
    "df_word_clusters.iloc[::222222].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      4304.000000\n",
       "mean         97.427277\n",
       "std        2518.074008\n",
       "min          15.000000\n",
       "25%          23.000000\n",
       "50%          37.000000\n",
       "75%          65.000000\n",
       "max      164979.000000\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_word_clusters.groupby('cluster_label')['word'].count()\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_label\n",
       "zikacarrying mosquito     80\n",
       "zombie                    35\n",
       "zoo                       29\n",
       "zoom                     151\n",
       "zuckerberg                91\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster_number</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24011</th>\n",
       "      <td>zombie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24016</th>\n",
       "      <td>corporate zombie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24022</th>\n",
       "      <td>zombie firmscompanie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76572</th>\n",
       "      <td>zombie outbreak</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176015</th>\n",
       "      <td>zombie virus</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182615</th>\n",
       "      <td>zombie star</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193144</th>\n",
       "      <td>zombie infectionsincluding prevention</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193150</th>\n",
       "      <td>zombie paper</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193152</th>\n",
       "      <td>actual fictional zombie literature</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199134</th>\n",
       "      <td>zombie army</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201778</th>\n",
       "      <td>brief zombie awakening</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215369</th>\n",
       "      <td>zombies</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215379</th>\n",
       "      <td>zombies original concept</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228438</th>\n",
       "      <td>giant zombie attack dog</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231289</th>\n",
       "      <td>zombie flick</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231761</th>\n",
       "      <td>zombie politician</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232715</th>\n",
       "      <td>sweet funny lighthearted zombie movie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236129</th>\n",
       "      <td>zombie basket</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247919</th>\n",
       "      <td>zombie internet</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253379</th>\n",
       "      <td>zombie food</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266976</th>\n",
       "      <td>socially minded zombie movie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269679</th>\n",
       "      <td>single zombie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275683</th>\n",
       "      <td>zombie connecteddevice army</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293430</th>\n",
       "      <td>zombie hunter</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313183</th>\n",
       "      <td>zombie movie army</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314835</th>\n",
       "      <td>zombie component</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319086</th>\n",
       "      <td>zombie rumor</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321986</th>\n",
       "      <td>zombie movie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322312</th>\n",
       "      <td>floating zombie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327276</th>\n",
       "      <td>tired zombie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352730</th>\n",
       "      <td>omnipresent zombie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354781</th>\n",
       "      <td>zombie text message</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355214</th>\n",
       "      <td>zombie bill</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368723</th>\n",
       "      <td>zombie chemical</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382058</th>\n",
       "      <td>natures microscopic zombie</td>\n",
       "      <td>2168</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         word  cluster_number cluster_label\n",
       "24011                                  zombie            2168        zombie\n",
       "24016                        corporate zombie            2168        zombie\n",
       "24022                    zombie firmscompanie            2168        zombie\n",
       "76572                         zombie outbreak            2168        zombie\n",
       "176015                           zombie virus            2168        zombie\n",
       "182615                            zombie star            2168        zombie\n",
       "193144  zombie infectionsincluding prevention            2168        zombie\n",
       "193150                           zombie paper            2168        zombie\n",
       "193152     actual fictional zombie literature            2168        zombie\n",
       "199134                            zombie army            2168        zombie\n",
       "201778                 brief zombie awakening            2168        zombie\n",
       "215369                                zombies            2168        zombie\n",
       "215379               zombies original concept            2168        zombie\n",
       "228438                giant zombie attack dog            2168        zombie\n",
       "231289                           zombie flick            2168        zombie\n",
       "231761                      zombie politician            2168        zombie\n",
       "232715  sweet funny lighthearted zombie movie            2168        zombie\n",
       "236129                          zombie basket            2168        zombie\n",
       "247919                        zombie internet            2168        zombie\n",
       "253379                            zombie food            2168        zombie\n",
       "266976           socially minded zombie movie            2168        zombie\n",
       "269679                          single zombie            2168        zombie\n",
       "275683            zombie connecteddevice army            2168        zombie\n",
       "293430                          zombie hunter            2168        zombie\n",
       "313183                      zombie movie army            2168        zombie\n",
       "314835                       zombie component            2168        zombie\n",
       "319086                           zombie rumor            2168        zombie\n",
       "321986                           zombie movie            2168        zombie\n",
       "322312                        floating zombie            2168        zombie\n",
       "327276                           tired zombie            2168        zombie\n",
       "352730                     omnipresent zombie            2168        zombie\n",
       "354781                    zombie text message            2168        zombie\n",
       "355214                            zombie bill            2168        zombie\n",
       "368723                        zombie chemical            2168        zombie\n",
       "382058             natures microscopic zombie            2168        zombie"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_clusters[df_word_clusters['cluster_label'] == \"zombie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_clusters.to_csv('./transition_files/word_cluster_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Replace text words with their cluster names (KeyWords)\n",
    "excluding \"noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
       "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
       "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
       "       'group_level_1', 'group_level_2', 'group_level_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['all_words'] = df_train['list_of_verb_lemmas'] + df_train['noun_phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419327\n",
      "419288\n"
     ]
    }
   ],
   "source": [
    "#delete \"noise\" \n",
    "print(len(df_word_clusters))\n",
    "df_word_clusters = df_word_clusters[df_word_clusters['cluster_label'] != \"nose\"]\n",
    "print(len(df_word_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster_label_dict =dict(zip(df_word_clusters['word'], df_word_clusters['cluster_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_key_words</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bound, led, noise, travel, noise, upgrade, travel, noise, noise, noise, sgt, noise, economic, noise, noise, noise, noise, surge, number, people, ...</td>\n",
       "      <td>[merging, led, wanting, travel, granted, upgrade, travel, apply, submit, streamline, scra, rise, big emerging economy, china, india, steady march,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[noise, noise, noise, gain, raised, noise, prevent, noise, noise, shift, noise, excited, save, noise, noise, promise, noise, getting, noise, tryin...</td>\n",
       "      <td>[rided, embracing, insists, gain, strengthen, improve, deterred, seeking, intends, shift, domiciled, rejoiced, saved, paid, outraged, promised, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[noise, end, noise, trying, lift, noise, reverse, cut, help, understand, upgrade, strike, wish, save, noise, trying, escape, noise, noise, pulled,...</td>\n",
       "      <td>[aised, ended, celebrate, tried, lift, forced, reverse, cut, help, understand, upgrade, strike, wish, save, spend, try, escape, slashing, encourag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[race, booked, noise, noise, control, demand, noise, noise, noise, noise, moving, upgrade, noise, noise, making, target, noise, ad, cruise, wave, ...</td>\n",
       "      <td>[race, booked, improve, announced, control, demand, peaking, piling, based, got, moving, upgrade, increase, announced, establish, aimed, based, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[noise, noise, noise, noise, noise, suggest, judgment, betting, expect, upgrade, noise, noise, noise, noise, buy, noise, believe, noise, noise, in...</td>\n",
       "      <td>[tart, caught, proved, reflected, like, suggest, judged, betting, expect, upgrade, weakens, having, pushed, tighten, buy, priced, doubt, tighten, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           all_key_words  \\\n",
       "0  [bound, led, noise, travel, noise, upgrade, travel, noise, noise, noise, sgt, noise, economic, noise, noise, noise, noise, surge, number, people, ...   \n",
       "1  [noise, noise, noise, gain, raised, noise, prevent, noise, noise, shift, noise, excited, save, noise, noise, promise, noise, getting, noise, tryin...   \n",
       "2  [noise, end, noise, trying, lift, noise, reverse, cut, help, understand, upgrade, strike, wish, save, noise, trying, escape, noise, noise, pulled,...   \n",
       "3  [race, booked, noise, noise, control, demand, noise, noise, noise, noise, moving, upgrade, noise, noise, making, target, noise, ad, cruise, wave, ...   \n",
       "4  [noise, noise, noise, noise, noise, suggest, judgment, betting, expect, upgrade, noise, noise, noise, noise, buy, noise, believe, noise, noise, in...   \n",
       "\n",
       "                                                                                                                                               all_words  \n",
       "0  [merging, led, wanting, travel, granted, upgrade, travel, apply, submit, streamline, scra, rise, big emerging economy, china, india, steady march,...  \n",
       "1  [rided, embracing, insists, gain, strengthen, improve, deterred, seeking, intends, shift, domiciled, rejoiced, saved, paid, outraged, promised, im...  \n",
       "2  [aised, ended, celebrate, tried, lift, forced, reverse, cut, help, understand, upgrade, strike, wish, save, spend, try, escape, slashing, encourag...  \n",
       "3  [race, booked, improve, announced, control, demand, peaking, piling, based, got, moving, upgrade, increase, announced, establish, aimed, based, ad...  \n",
       "4  [tart, caught, proved, reflected, like, suggest, judged, betting, expect, upgrade, weakens, having, pushed, tighten, buy, priced, doubt, tighten, ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['all_key_words'] = df_train['all_words'].apply(lambda Wlist: \n",
    "                                                        [word_cluster_label_dict[w] for w in  Wlist\n",
    "                                                                                    if w in word_cluster_label_dict\n",
    "                                                        ])\n",
    "df_train[['all_key_words', 'all_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./transition_files/df_train_for_LDA.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(df_train, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GLG - 3_step - Assigning Text Groups.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.45px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
