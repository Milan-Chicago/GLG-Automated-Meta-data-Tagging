{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea:\n",
    "Our solution: LDA + BERT based embeddings of noun phrases and verbs :\n",
    "- Each noun phrase and verb in the texts is  transformed to embedding vector using Universal Sentence Encoder (transformer based on BERT)\n",
    "- Embedding vectors from (a) are clustered (HDBSCAN + UNET)\n",
    "- Words/phrases with embedding vectors closest to the centers of resulting clusters form key word/phrase\n",
    "- Each text in the training sample is converted to collection of key-phrases by replacing its noun phrases and verbs with keyword/phrases and deleting other words\n",
    "- LDA is performed on the transformed texts\n",
    "\n",
    "\n",
    "**Reference:**<br>\n",
    "- Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Céspedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. **Universal Sentence Encoder.** *arXiv:1803.11175, 2018.*\n",
    "- McInnes, L, Healy, J, **UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction**, *ArXiv e-prints 1802.03426, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clNUUp3MUO2t"
   },
   "source": [
    "# Load data and python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1622131163361,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "7dYQIbH6UO2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "module https://tfhub.dev/google/universal-sentence-encoder-large/5 loaded\n"
     ]
    }
   ],
   "source": [
    "# data processing libraries\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# display wider columns in pandas data frames where necessary\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "#Load the Universal Sentence Encoder's TF Hub module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "\n",
    "import umap\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (33982, 12)\n",
      "df_train.shape: Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
      "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
      "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./transition_files/train.tsv\", sep='\\t')\n",
    "print(\"df_train.shape:\", df_train.shape)\n",
    "print(\"df_train.shape:\",df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bxk10XYUqNp"
   },
   "source": [
    "# Getting text clusters through sentence embedding comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1622131190534,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "MsgF9abaX2Bn"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(df_data, column = \"word\", N_batches=1):\n",
    "    #split data into N batches\n",
    "    N = N_batches\n",
    "\n",
    "    part = int(len(df_data)/N)\n",
    "    print(N, \"batches with\", part + 1, column + \"s each\")\n",
    "\n",
    "    #get embeddings for each N words\n",
    "    index = 0\n",
    "    batch_num = 0\n",
    "    list_dfs = []\n",
    "\n",
    "    while index < len(df_data): \n",
    "        df_tmp = df_data.iloc[index : index + part].copy()\n",
    "        df_tmp = df_tmp.reset_index(drop=True)\n",
    "        print (\"Batch number:\", batch_num + 1, \"out of \", N)\n",
    "\n",
    "        df_batch_embeddings = pd.DataFrame(get_embeddings(list(df_tmp[column])).numpy())\n",
    "\n",
    "        num_embeddings = df_batch_embeddings.shape[1]\n",
    "        columns = [\"emb_\" + str(i) for i in range(512)]\n",
    "        df_tmp[columns] = df_batch_embeddings\n",
    "\n",
    "        list_dfs.append(df_tmp)\n",
    "        batch_num = batch_num + 1\n",
    "        index = index + part\n",
    "\n",
    "    #concatinate batches into single dataset\n",
    "    df_emb = pd.concat(list_dfs)\n",
    "\n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [rise, big emerging economy, china, india, steady march, globalisation, surge, number, people, business, tourism, result, demand, visa, unpreceden...\n",
       "1    [pfizer, commitment, corporate social responsibility csr, drugs giant talk, responsibility, society, world, access, product, work, ngos, global he...\n",
       "2    [week, federal reserve, interest rate, time, year, world, central bank, rate, recent year, long spell, course, chart, outcome, americas rate rise,...\n",
       "3    [cruise line, wave, year, nearly, holiday, sea, result, december 18th carnival, worlds largest operator, global market, fullyear earning, demand, ...\n",
       "4    [investors, calendar year, buoyant mood, unexpected event, consensus, respect, view, investor, market price, column, potential surprise, definitio...\n",
       "Name: noun_phrases, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['noun_phrases'] = df_train['noun_phrases'].str[2:-2]\n",
    "df_train['noun_phrases'] = df_train['noun_phrases'].str.lower().str.split(\"', '\")\n",
    "df_train['noun_phrases'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['rise', 'big emerging economy', 'china', 'india', 'steady march'], 1417049)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_NPs = list(df_train['noun_phrases'])\n",
    "all_NPs = [np for l in all_NPs for np in l if len(np)>0]\n",
    "all_NPs[:5], len(all_NPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[emerging, led, wanting, travel, granted, Upgrade, travel, apply, submit, streamline, scrap]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['list_of_verb_lemmas'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                               [merging, led, wanting, travel, granted, upgrade, travel, apply, submit, streamline, scra]\n",
       "1    [rided, embracing, insists, gain, strengthen, improve, deterred, seeking, intends, shift, domiciled, rejoiced, saved, paid, outraged, promised, im...\n",
       "2    [aised, ended, celebrate, tried, lift, forced, reverse, cut, help, understand, upgrade, strike, wish, save, spend, try, escape, slashing, encourag...\n",
       "3      [race, booked, improve, announced, control, demand, peaking, piling, based, got, moving, upgrade, increase, announced, establish, aimed, based, ad]\n",
       "4    [tart, caught, proved, reflected, like, suggest, judged, betting, expect, upgrade, weakens, having, pushed, tighten, buy, priced, doubt, tighten, ...\n",
       "Name: list_of_verb_lemmas, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['list_of_verb_lemmas'] = df_train['list_of_verb_lemmas'].str[2:-2]\n",
    "df_train['list_of_verb_lemmas'] = df_train['list_of_verb_lemmas'].str.lower().str.split(\", \")\n",
    "df_train['list_of_verb_lemmas'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['merging', 'led', 'wanting', 'travel', 'granted'], 675330)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_Vs = list(df_train['list_of_verb_lemmas'])\n",
    "all_Vs = [v for l in all_Vs for v in l if len(v)>0]\n",
    "all_Vs[:5], len(all_Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419327"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words =  list(set(all_NPs + all_Vs))\n",
    "len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_QElB-4UM2z",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sara cunningham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impediment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teenage love story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrongful death lawsuit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>collective right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word\n",
       "0         sara cunningham\n",
       "1              impediment\n",
       "2      teenage love story\n",
       "3  wrongful death lawsuit\n",
       "4        collective right"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.DataFrame({'word': all_words})\n",
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 batches with 4194 words each\n",
      "Batch number: 1 out of  100\n",
      "Batch number: 2 out of  100\n",
      "Batch number: 3 out of  100\n",
      "Batch number: 4 out of  100\n",
      "Batch number: 5 out of  100\n",
      "Batch number: 6 out of  100\n",
      "Batch number: 7 out of  100\n",
      "Batch number: 8 out of  100\n",
      "Batch number: 9 out of  100\n",
      "Batch number: 10 out of  100\n",
      "Batch number: 11 out of  100\n",
      "Batch number: 12 out of  100\n",
      "Batch number: 13 out of  100\n",
      "Batch number: 14 out of  100\n",
      "Batch number: 15 out of  100\n",
      "Batch number: 16 out of  100\n",
      "Batch number: 17 out of  100\n",
      "Batch number: 18 out of  100\n",
      "Batch number: 19 out of  100\n",
      "Batch number: 20 out of  100\n",
      "Batch number: 21 out of  100\n",
      "Batch number: 22 out of  100\n",
      "Batch number: 23 out of  100\n",
      "Batch number: 24 out of  100\n",
      "Batch number: 25 out of  100\n",
      "Batch number: 26 out of  100\n",
      "Batch number: 27 out of  100\n",
      "Batch number: 28 out of  100\n",
      "Batch number: 29 out of  100\n",
      "Batch number: 30 out of  100\n",
      "Batch number: 31 out of  100\n",
      "Batch number: 32 out of  100\n",
      "Batch number: 33 out of  100\n",
      "Batch number: 34 out of  100\n",
      "Batch number: 35 out of  100\n",
      "Batch number: 36 out of  100\n",
      "Batch number: 37 out of  100\n",
      "Batch number: 38 out of  100\n",
      "Batch number: 39 out of  100\n",
      "Batch number: 40 out of  100\n",
      "Batch number: 41 out of  100\n",
      "Batch number: 42 out of  100\n",
      "Batch number: 43 out of  100\n",
      "Batch number: 44 out of  100\n",
      "Batch number: 45 out of  100\n",
      "Batch number: 46 out of  100\n",
      "Batch number: 47 out of  100\n",
      "Batch number: 48 out of  100\n",
      "Batch number: 49 out of  100\n",
      "Batch number: 50 out of  100\n",
      "Batch number: 51 out of  100\n",
      "Batch number: 52 out of  100\n",
      "Batch number: 53 out of  100\n",
      "Batch number: 54 out of  100\n",
      "Batch number: 55 out of  100\n",
      "Batch number: 56 out of  100\n",
      "Batch number: 57 out of  100\n",
      "Batch number: 58 out of  100\n",
      "Batch number: 59 out of  100\n",
      "Batch number: 60 out of  100\n",
      "Batch number: 61 out of  100\n",
      "Batch number: 62 out of  100\n",
      "Batch number: 63 out of  100\n",
      "Batch number: 64 out of  100\n",
      "Batch number: 65 out of  100\n",
      "Batch number: 66 out of  100\n",
      "Batch number: 67 out of  100\n",
      "Batch number: 68 out of  100\n",
      "Batch number: 69 out of  100\n",
      "Batch number: 70 out of  100\n",
      "Batch number: 71 out of  100\n",
      "Batch number: 72 out of  100\n",
      "Batch number: 73 out of  100\n",
      "Batch number: 74 out of  100\n",
      "Batch number: 75 out of  100\n",
      "Batch number: 76 out of  100\n",
      "Batch number: 77 out of  100\n",
      "Batch number: 78 out of  100\n",
      "Batch number: 79 out of  100\n",
      "Batch number: 80 out of  100\n",
      "Batch number: 81 out of  100\n",
      "Batch number: 82 out of  100\n",
      "Batch number: 83 out of  100\n",
      "Batch number: 84 out of  100\n",
      "Batch number: 85 out of  100\n",
      "Batch number: 86 out of  100\n",
      "Batch number: 87 out of  100\n",
      "Batch number: 88 out of  100\n",
      "Batch number: 89 out of  100\n",
      "Batch number: 90 out of  100\n",
      "Batch number: 91 out of  100\n",
      "Batch number: 92 out of  100\n",
      "Batch number: 93 out of  100\n",
      "Batch number: 94 out of  100\n",
      "Batch number: 95 out of  100\n",
      "Batch number: 96 out of  100\n",
      "Batch number: 97 out of  100\n",
      "Batch number: 98 out of  100\n",
      "Batch number: 99 out of  100\n",
      "Batch number: 100 out of  100\n",
      "Batch number: 101 out of  100\n",
      "CPU times: user 50min 29s, sys: 2min 17s, total: 52min 46s\n",
      "Wall time: 5min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_502</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sara cunningham</td>\n",
       "      <td>-0.020924</td>\n",
       "      <td>-0.029926</td>\n",
       "      <td>0.020496</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>-0.023925</td>\n",
       "      <td>-0.032119</td>\n",
       "      <td>-0.032938</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>-0.057764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006286</td>\n",
       "      <td>-0.095762</td>\n",
       "      <td>-0.046372</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>-0.017343</td>\n",
       "      <td>0.038016</td>\n",
       "      <td>-0.047309</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>-0.039049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impediment</td>\n",
       "      <td>0.040666</td>\n",
       "      <td>-0.015920</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>-0.043400</td>\n",
       "      <td>0.020426</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028403</td>\n",
       "      <td>-0.105091</td>\n",
       "      <td>-0.007806</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>-0.070051</td>\n",
       "      <td>0.036601</td>\n",
       "      <td>0.018366</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>-0.080449</td>\n",
       "      <td>-0.030747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teenage love story</td>\n",
       "      <td>-0.078461</td>\n",
       "      <td>-0.064888</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.034558</td>\n",
       "      <td>-0.163979</td>\n",
       "      <td>0.019794</td>\n",
       "      <td>0.010290</td>\n",
       "      <td>-0.007642</td>\n",
       "      <td>0.015894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025725</td>\n",
       "      <td>0.060196</td>\n",
       "      <td>-0.050020</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>-0.027109</td>\n",
       "      <td>0.039043</td>\n",
       "      <td>-0.050389</td>\n",
       "      <td>-0.055588</td>\n",
       "      <td>-0.003846</td>\n",
       "      <td>0.013916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrongful death lawsuit</td>\n",
       "      <td>-0.050084</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>-0.020140</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>-0.001983</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.014738</td>\n",
       "      <td>0.027472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>-0.053382</td>\n",
       "      <td>0.081420</td>\n",
       "      <td>0.019674</td>\n",
       "      <td>-0.018711</td>\n",
       "      <td>0.053163</td>\n",
       "      <td>-0.044375</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>-0.022772</td>\n",
       "      <td>-0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>collective right</td>\n",
       "      <td>0.011474</td>\n",
       "      <td>0.017047</td>\n",
       "      <td>-0.018245</td>\n",
       "      <td>-0.044409</td>\n",
       "      <td>-0.013463</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>0.066072</td>\n",
       "      <td>-0.058078</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013454</td>\n",
       "      <td>-0.055200</td>\n",
       "      <td>0.069321</td>\n",
       "      <td>-0.101623</td>\n",
       "      <td>0.047158</td>\n",
       "      <td>-0.004187</td>\n",
       "      <td>0.037549</td>\n",
       "      <td>-0.011713</td>\n",
       "      <td>0.023572</td>\n",
       "      <td>0.017899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word     emb_0     emb_1     emb_2     emb_3     emb_4  \\\n",
       "0         sara cunningham -0.020924 -0.029926  0.020496  0.056778 -0.023925   \n",
       "1              impediment  0.040666 -0.015920  0.007230  0.001477 -0.043400   \n",
       "2      teenage love story -0.078461 -0.064888  0.001819  0.034558 -0.163979   \n",
       "3  wrongful death lawsuit -0.050084  0.020163  0.001580 -0.020140  0.004819   \n",
       "4        collective right  0.011474  0.017047 -0.018245 -0.044409 -0.013463   \n",
       "\n",
       "      emb_5     emb_6     emb_7     emb_8  ...   emb_502   emb_503   emb_504  \\\n",
       "0 -0.032119 -0.032938  0.044124 -0.057764  ... -0.006286 -0.095762 -0.046372   \n",
       "1  0.020426  0.046355  0.031863  0.022177  ... -0.028403 -0.105091 -0.007806   \n",
       "2  0.019794  0.010290 -0.007642  0.015894  ... -0.025725  0.060196 -0.050020   \n",
       "3 -0.001983  0.014155  0.014738  0.027472  ...  0.032300 -0.053382  0.081420   \n",
       "4 -0.001805  0.066072 -0.058078  0.009794  ... -0.013454 -0.055200  0.069321   \n",
       "\n",
       "    emb_505   emb_506   emb_507   emb_508   emb_509   emb_510   emb_511  \n",
       "0  0.005935 -0.017343  0.038016 -0.047309  0.078175  0.005584 -0.039049  \n",
       "1 -0.008843 -0.070051  0.036601  0.018366  0.008329 -0.080449 -0.030747  \n",
       "2 -0.011564 -0.027109  0.039043 -0.050389 -0.055588 -0.003846  0.013916  \n",
       "3  0.019674 -0.018711  0.053163 -0.044375  0.009313 -0.022772 -0.003754  \n",
       "4 -0.101623  0.047158 -0.004187  0.037549 -0.011713  0.023572  0.017899  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#creating word2vec matrix\n",
    "df_w2v = get_word_embeddings(df_words, column = \"word\", N_batches=100)\n",
    "df_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_502</th>\n",
       "      <th>emb_503</th>\n",
       "      <th>emb_504</th>\n",
       "      <th>emb_505</th>\n",
       "      <th>emb_506</th>\n",
       "      <th>emb_507</th>\n",
       "      <th>emb_508</th>\n",
       "      <th>emb_509</th>\n",
       "      <th>emb_510</th>\n",
       "      <th>emb_511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sara cunningham</td>\n",
       "      <td>-0.020924</td>\n",
       "      <td>-0.029926</td>\n",
       "      <td>0.020496</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>-0.023925</td>\n",
       "      <td>-0.032119</td>\n",
       "      <td>-0.032938</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>-0.057764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006286</td>\n",
       "      <td>-0.095762</td>\n",
       "      <td>-0.046372</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>-0.017343</td>\n",
       "      <td>0.038016</td>\n",
       "      <td>-0.047309</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>-0.039049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>director ridley scotts space survival film</td>\n",
       "      <td>0.030060</td>\n",
       "      <td>0.061103</td>\n",
       "      <td>0.020915</td>\n",
       "      <td>-0.005141</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0.014103</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.017706</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036375</td>\n",
       "      <td>-0.049007</td>\n",
       "      <td>-0.059152</td>\n",
       "      <td>-0.000596</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.023513</td>\n",
       "      <td>-0.004283</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>-0.024940</td>\n",
       "      <td>0.054678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>new father</td>\n",
       "      <td>0.026679</td>\n",
       "      <td>-0.004128</td>\n",
       "      <td>-0.052970</td>\n",
       "      <td>-0.037194</td>\n",
       "      <td>0.032533</td>\n",
       "      <td>-0.015992</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>0.024973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054416</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>-0.040044</td>\n",
       "      <td>0.008331</td>\n",
       "      <td>0.043501</td>\n",
       "      <td>0.052999</td>\n",
       "      <td>-0.026388</td>\n",
       "      <td>-0.008315</td>\n",
       "      <td>0.068394</td>\n",
       "      <td>0.017688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            word     emb_0     emb_1  \\\n",
       "0                                sara cunningham -0.020924 -0.029926   \n",
       "3246  director ridley scotts space survival film  0.030060  0.061103   \n",
       "2299                                  new father  0.026679 -0.004128   \n",
       "\n",
       "         emb_2     emb_3     emb_4     emb_5     emb_6     emb_7     emb_8  \\\n",
       "0     0.020496  0.056778 -0.023925 -0.032119 -0.032938  0.044124 -0.057764   \n",
       "3246  0.020915 -0.005141  0.004050  0.014103  0.008665  0.017706  0.009135   \n",
       "2299 -0.052970 -0.037194  0.032533 -0.015992  0.042322 -0.070251  0.024973   \n",
       "\n",
       "      ...   emb_502   emb_503   emb_504   emb_505   emb_506   emb_507  \\\n",
       "0     ... -0.006286 -0.095762 -0.046372  0.005935 -0.017343  0.038016   \n",
       "3246  ...  0.036375 -0.049007 -0.059152 -0.000596  0.008104  0.023513   \n",
       "2299  ... -0.054416  0.026000 -0.040044  0.008331  0.043501  0.052999   \n",
       "\n",
       "       emb_508   emb_509   emb_510   emb_511  \n",
       "0    -0.047309  0.078175  0.005584 -0.039049  \n",
       "3246 -0.004283 -0.001532 -0.024940  0.054678  \n",
       "2299 -0.026388 -0.008315  0.068394  0.017688  \n",
       "\n",
       "[3 rows x 513 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v.iloc[::150001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction \n",
    "UMAP https://github.com/lmcinnes/umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419327, 512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"emb_\" + str(i) for i in range(512)]\n",
    "embeddings = df_w2v[columns].values\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56min 9s, sys: 1min 21s, total: 57min 31s\n",
      "Wall time: 6min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(419327, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)\n",
    "umap_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419327,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster labels\n",
    "labels = cluster.labels_\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 4317\n"
     ]
    }
   ],
   "source": [
    "#number of clusters (key-words/phrases)\n",
    "print(\"Number of clusters:\",labels.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next:\n",
    "- defining key words\n",
    "- replacing texts with list of key-words\n",
    "\n",
    "**Note:**\n",
    "Clusters in (H)DBSCAN do not have centers.\n",
    "\n",
    "The clusters may be non-convex, and if you compute the average of all points (and your data are points - they don't need to be) it may then be outside of the cluster.\n",
    "\n",
    "Also note that DBSCAN also gives noise points,that don't have a center at all.\n",
    "\n",
    "\n",
    "***\n",
    "- running LDA\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GLG - 3_step - Assigning Text Groups.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.455px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
